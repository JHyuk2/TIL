{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".py 파일의 개수: 5818\n",
      "분할된 .py 파일의 개수: 10177\n",
      ".mdx 파일의 개수: 272\n",
      "분할된 .mdx 파일의 개수: 597\n",
      "분할된 .df 파일 개수: 825\n",
      "총 도큐먼트 개수((teddy, py, mdx)): 11599\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import Language\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Python 파일을 로드하고 문서를 분할합니다.\n",
    "repo_root = \"/Users/cosmos/Desktop/ai/langchain/libs\"\n",
    "repo_core = repo_root + \"/core/langchain_core\"\n",
    "repo_community = repo_root + \"/community/langchain_community\"\n",
    "repo_experimental = repo_root + \"/experimental/langchain_experimental\"\n",
    "repo_partners = repo_root + \"/partners\"\n",
    "repo_text_splitter = repo_root + \"/text_splitters/langchain_text_splitters\"\n",
    "repo_cookbook = repo_root + \"/cookbook\"\n",
    "\n",
    "py_documents = []\n",
    "for path in [repo_core, repo_community, repo_experimental, repo_partners, repo_cookbook]:\n",
    "    loader = GenericLoader.from_filesystem(\n",
    "        path, glob=\"**/*\", suffixes=[\".py\"],\n",
    "        parser=LanguageParser(language=Language.PYTHON, parser_threshold=30),\n",
    "    )\n",
    "    py_documents.extend(loader.load())\n",
    "print(f\".py 파일의 개수: {len(py_documents)}\")\n",
    "\n",
    "py_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=2000, chunk_overlap=200\n",
    ")\n",
    "py_docs = py_splitter.split_documents(py_documents)\n",
    "print(f\"분할된 .py 파일의 개수: {len(py_docs)}\")\n",
    "\n",
    "# MDX 파일을 로드하고 문서를 분할합니다.\n",
    "root_dir = \"/Users/cosmos/Desktop/ai/langchain/\"\n",
    "\n",
    "mdx_documents = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        if (file.endswith(\".mdx\")) and \"*venv/\" not in dirpath:\n",
    "            try:\n",
    "                loader = TextLoader(os.path.join(dirpath, file), encoding=\"utf-8\")\n",
    "                mdx_documents.extend(loader.load())\n",
    "            except Exception:\n",
    "                pass\n",
    "print(f\".mdx 파일의 개수: {len(mdx_documents)}\")\n",
    "\n",
    "mdx_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "mdx_docs = mdx_splitter.split_documents(mdx_documents)\n",
    "print(f\"분할된 .mdx 파일의 개수: {len(mdx_docs)}\")\n",
    "\n",
    "# Teddy님의 랭체인노트를 로드하고 문서를 분할합니다.\n",
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "\n",
    "df = pd.read_csv('data_list_with_content.csv')\n",
    "df_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "teddy_docs = []\n",
    "for index, row in df.iterrows():\n",
    "    if pd.isna(row['content']):\n",
    "        continue\n",
    "    chunks = df_splitter.split_text(row['content'])\n",
    "    for chunk in chunks:\n",
    "        teddy_docs.append(Document(page_content=chunk, metadata={\"title\": row['title'], \"source\": row['source']}))\n",
    "print(f\"분할된 .df 파일 개수: {len(teddy_docs)}\")\n",
    "\n",
    "# 파이썬 문서, MDX 문서, PDF 문서, 테디노트(Langhchin-KR) 문서를 결합합니다.\n",
    "combined_documents = py_docs + mdx_docs + teddy_docs\n",
    "print(f\"총 도큐먼트 개수((teddy, py, mdx)): {len(combined_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 필요한 임베딩과 캐싱설정을 수행합니다. \n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "store = LocalFileStore(\"./cache/\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", disallowed_special=())\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, store, namespace=embeddings.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiwi Tokenizer를 설정합니다.\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "kiwi = Kiwi()\n",
    "def kiwi_tokenize(text):\n",
    "    return [token.form for token in kiwi.tokenize(text)]\n",
    "\n",
    "# FAISS 클래스를 가져와 검색 모델 인스턴스를 생성합니다.\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "\n",
    "FAISS_DB_INDEX = \"langchain_faiss\"\n",
    "db = FAISS.from_documents(combined_documents, cached_embeddings)\n",
    "faiss_retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10})\n",
    "\n",
    "# BM25Retriever 클래스를 가져와 검색 모델 인스턴스를 생성합니다.\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(combined_documents, preprocess_func=kiwi_tokenize, k=10)\n",
    "\n",
    "# EnsembleRetriever 클래스를 사용하여 검색 모델을 결합하여 사용합니다.\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever],\n",
    "    weights=[0.6, 0.4], search_type=\"mmr\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt를 설정합니다.\n",
    "from langchain_core.prompts import (\n",
    "    PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    ")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "당신은 20년차 AI 개발자입니다. 당신의 임무는 주어진 질문에 대하여 최대한 문서의 정보를 활용하여 답변하는 것입니다.\n",
    "문서는 Python 코드에 대한 정보를 담고 있습니다. 따라서, 답변을 작성할 때에는 Python 코드에 대한 상세한 code snippet을 포함하여 작성해주세요.\n",
    "최대한 자세하게 답변하고, 한글로 답변해 주세요. 주어진 문서에서 답변을 찾을 수 없는 경우, \"문서에 답변이 없습니다.\"라고 답변해 주세요.\n",
    "답변에 대한 출처(source)를 반드시 표기해야 합니다. 출처(source)는 당신이 답변을 만들때 참조한 문서의 metadata의 source를 표기해 주세요.\n",
    "\n",
    "# 주어진 문서:\n",
    "{context}\n",
    "\n",
    "# 질문:\n",
    "{question}\n",
    "\n",
    "# 답변: \n",
    "\n",
    "# 출처:\n",
    "- source1\n",
    "- source2\n",
    "- ...\n",
    "\"\"\"\n",
    "def get_context(query):\n",
    "    relevant_docs = ensemble_retriever.invoke(query)\n",
    "    return relevant_docs\n",
    "\n",
    "system_template = SystemMessagePromptTemplate.from_template(prompt_template)\n",
    "system_message = system_template.format(context=get_context(\"{question}\"), question=\"{question}\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Callback, LLM, Chain를 설정합니다.\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "class StreamCallback(BaseCallbackHandler):\n",
    "     def on_llm_new_token(self, token: str, **kwargs): print(token, end=\"\", flush=True)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, streaming=True, verbose=True, callbacks=[StreamCallback()])\n",
    "\n",
    "# 대화기록 유지를 위한 함수입니다.\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "runnable = prompt | llm | StrOutputParser()\n",
    "\n",
    "store = {}  # 세션 기록을 저장할 딕셔너리\n",
    "\n",
    "# 세션 ID를 기반으로 세션 기록을 가져오는 함수\n",
    "def get_session_history(session_ids: str) -> BaseChatMessageHistory:\n",
    "    print(session_ids)\n",
    "    if session_ids not in store:  # 세션 ID가 store에 없는 경우\n",
    "        store[session_ids] = ChatMessageHistory()\n",
    "    return store[session_ids]  # 해당 세션 ID에 대한 세션 기록 반환\n",
    "\n",
    "rag_chain = RunnableWithMessageHistory(  # RunnableWithMessageHistory 객체 생성\n",
    "        runnable,  # 실행할 Runnable 객체\n",
    "        get_session_history,  # 세션 기록을 가져오는 함수\n",
    "        input_messages_key=\"question\",  # 입력 메시지의 키\n",
    "        history_messages_key=\"history\",  # 기록 메시지의 키\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ensemble_retriver]\n",
      "https://wikidocs.net/234017\n",
      "https://wikidocs.net/234475\n",
      "https://wikidocs.net/234018\n",
      "https://wikidocs.net/234021\n",
      "https://wikidocs.net/235703\n",
      "https://wikidocs.net/234018\n",
      "https://wikidocs.net/233325\n",
      "https://wikidocs.net/234018\n",
      "https://wikidocs.net/234475\n",
      "https://wikidocs.net/235886\n",
      "https://wikidocs.net/233351\n",
      "https://wikidocs.net/233350\n",
      "https://wikidocs.net/234020\n",
      "https://wikidocs.net/233790\n",
      "https://wikidocs.net/233350\n",
      "https://wikidocs.net/235704\n",
      "https://wikidocs.net/234021\n",
      "https://wikidocs.net/233791\n",
      "https://wikidocs.net/233798\n",
      "https://wikidocs.net/233804\n"
     ]
    }
   ],
   "source": [
    "# Retriever 문서 점검, 리트리버 조정 및 문서검색 확인\n",
    "\n",
    "# faiss_retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10})\n",
    "# bm25_retriever = BM25Retriever.from_documents(combined_documents, preprocess_func=kiwi_tokenize, k=10)\n",
    "\n",
    "# ensemble_retriever = EnsembleRetriever(\n",
    "#     retrievers=[bm25_retriever, faiss_retriever],\n",
    "#     weights=[0.4, 0.6], search_type=\"mmr\",\n",
    "# )\n",
    "\n",
    "query = \"Prompt Temlate에 대해 설명해줘!\"\n",
    "documents = ensemble_retriever.invoke(query)\n",
    "print(\"[ensemble_retriver]\")\n",
    "for doc in documents : print(doc.metadata['source'])\n",
    "# for doc in documents : print(doc), print(\"\\n\")\n",
    "\n",
    "# print(\"\\n[faiss_retriver]\")\n",
    "# documents = faiss_retriever.invoke(query)\n",
    "# for doc in documents : print(doc.metadata['source'])\n",
    "\n",
    "# print(\"\\n[chroma_retriver]\")\n",
    "# documents = chroma_retriever.invoke(query)\n",
    "# for doc in documents : print(doc.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\n",
      "Prompt Template은 자연어 처리(NLP) 모델, 특히 언어 모델을 사용할 때 입력으로 제공되는 텍스트의 형식을 정의하는 템플릿입니다. Prompt Template은 모델이 특정 문맥에서 작동하도록 설정하고, 제공된 정보를 바탕으로 보다 정확하고 관련성 높은 답변을 생성할 수 있도록 돕습니다. Prompt Template은 주로 다음과 같은 요소로 구성됩니다:\n",
      "\n",
      "1. **지시사항(Instruction)**: 모델에게 주어진 작업을 설명하는 부분입니다. 예를 들어, \"질문에 답하세요\" 또는 \"다음 문장을 번역하세요\"와 같은 지시사항이 포함될 수 있습니다.\n",
      "\n",
      "2. **질문(Question)**: 사용자가 입력한 질문이나 명령입니다. 이 부분은 모델이 처리해야 할 실제 입력입니다.\n",
      "\n",
      "3. **문맥(Context)**: 모델이 질문에 답하기 위해 사용할 수 있는 추가 정보입니다. 이는 검색된 문서나 데이터베이스에서 가져온 정보일 수 있습니다.\n",
      "\n",
      "### 예제\n",
      "다음은 Prompt Template의 예제입니다:\n",
      "\n",
      "```python\n",
      "template = \"\"\"\n",
      "당신은 질문-답변(Question-Answer) Task를 수행하는 AI 어시스턴트입니다.\n",
      "검색된 문맥(context)를 사용하여 질문(question)에 답하세요.\n",
      "만약, 문맥(context)으로부터 답을 찾을 수 없다면 '모른다'고 말하세요.\n",
      "한국어로 대답하세요.\n",
      "\n",
      "# Question: {question}\n",
      "# Context: {context}\n",
      "\"\"\"\n",
      "```\n",
      "\n",
      "이 템플릿은 모델에게 질문과 문맥을 제공하고, 이를 바탕으로 답변을 생성하도록 지시합니다.\n",
      "\n",
      "### 사용 예제\n",
      "Prompt Template을 사용하여 실제로 모델을 호출하는 예제는 다음과 같습니다:\n",
      "\n",
      "```python\n",
      "from langchain_core.prompts import ChatPromptTemplate\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "# 템플릿을 정의합니다.\n",
      "template = \"\"\"\n",
      "당신은 질문-답변(Question-Answer) Task를 수행하는 AI 어시스턴트입니다.\n",
      "검색된 문맥(context)를 사용하여 질문(question)에 답하세요.\n",
      "만약, 문맥(context)으로부터 답을 찾을 수 없다면 '모른다'고 말하세요.\n",
      "한국어로 대답하세요.\n",
      "\n",
      "# Question: {question}\n",
      "# Context: {context}\n",
      "\"\"\"\n",
      "\n",
      "# 템플릿으로부터 채팅 프롬프트를 생성합니다.\n",
      "prompt = ChatPromptTemplate.from_template(template)\n",
      "\n",
      "# ChatOpenAI 모델을 초기화합니다.\n",
      "model = ChatOpenAI()\n",
      "\n",
      "# 검색 체인을 구성합니다.\n",
      "retrieval_chain = (\n",
      "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
      "    | prompt\n",
      "    | model\n",
      "    | StrOutputParser()\n",
      ")\n",
      "\n",
      "# 검색 체인을 실행하여 질문에 대한 답변을 얻습니다.\n",
      "response = retrieval_chain.invoke({\"question\": \"대한민국의 수도는 어디인가요?\", \"context\": \"서울은 대한민국의 수도입니다.\"})\n",
      "print(response)\n",
      "```\n",
      "\n",
      "이 예제에서는 `ChatPromptTemplate`을 사용하여 템플릿을 정의하고, `ChatOpenAI` 모델을 초기화한 후, 검색 체인을 구성하여 질문에 대한 답변을 얻습니다.\n",
      "\n",
      "### 참고 문서\n",
      "- [LangChain Hub](https://wikidocs.net/233349)\n",
      "- [LangChain Prompts](https://wikidocs.net/233347)\n",
      "\n",
      "### 출처\n",
      "- [LangChain Hub](https://wikidocs.net/233349)\n",
      "- [LangChain Prompts](https://wikidocs.net/233347)"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"question\": query}, config={\"configurable\": {\"session_id\": \"foo\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\n",
      "1번 항목인 **지시사항(Instruction)**에 대해 자세히 설명드리겠습니다.\n",
      "\n",
      "### 지시사항(Instruction)\n",
      "\n",
      "지시사항은 Prompt Template의 핵심 요소 중 하나로, 모델에게 주어진 작업을 명확하게 설명하는 부분입니다. 지시사항은 모델이 특정 작업을 수행할 때 필요한 지침을 제공하며, 모델이 올바른 방식으로 응답을 생성할 수 있도록 돕습니다. 지시사항은 다음과 같은 역할을 합니다:\n",
      "\n",
      "1. **작업의 명확화**: 모델이 수행해야 할 작업을 명확하게 정의합니다. 예를 들어, 질문에 답변하는 것인지, 텍스트를 번역하는 것인지, 요약하는 것인지 등을 명확히 합니다.\n",
      "2. **문맥 설정**: 모델이 어떤 문맥에서 작동해야 하는지 설정합니다. 이를 통해 모델은 제공된 정보를 바탕으로 보다 정확하고 관련성 높은 답변을 생성할 수 있습니다.\n",
      "3. **응답 형식 지정**: 모델이 생성해야 할 응답의 형식을 지정합니다. 예를 들어, 응답이 간결해야 하는지, 특정 형식을 따라야 하는지 등을 지시할 수 있습니다.\n",
      "\n",
      "### 예제\n",
      "\n",
      "다음은 지시사항을 포함한 Prompt Template의 예제입니다:\n",
      "\n",
      "```python\n",
      "template = \"\"\"\n",
      "당신은 질문-답변(Question-Answer) Task를 수행하는 AI 어시스턴트입니다.\n",
      "검색된 문맥(context)를 사용하여 질문(question)에 답하세요.\n",
      "만약, 문맥(context)으로부터 답을 찾을 수 없다면 '모른다'고 말하세요.\n",
      "한국어로 대답하세요.\n",
      "\n",
      "# Question: {question}\n",
      "# Context: {context}\n",
      "\"\"\"\n",
      "```\n",
      "\n",
      "이 템플릿에서 지시사항은 다음과 같습니다:\n",
      "- \"당신은 질문-답변(Question-Answer) Task를 수행하는 AI 어시스턴트입니다.\"\n",
      "- \"검색된 문맥(context)를 사용하여 질문(question)에 답하세요.\"\n",
      "- \"만약, 문맥(context)으로부터 답을 찾을 수 없다면 '모른다'고 말하세요.\"\n",
      "- \"한국어로 대답하세요.\"\n",
      "\n",
      "이 지시사항들은 모델이 질문에 답변할 때 어떤 정보를 사용해야 하고, 답변을 찾을 수 없을 때 어떻게 응답해야 하는지, 그리고 응답의 언어를 명확히 지정하고 있습니다.\n",
      "\n",
      "### 사용 예제\n",
      "\n",
      "지시사항을 포함한 Prompt Template을 사용하여 실제로 모델을 호출하는 예제는 다음과 같습니다:\n",
      "\n",
      "```python\n",
      "from langchain_core.prompts import ChatPromptTemplate\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "# 템플릿을 정의합니다.\n",
      "template = \"\"\"\n",
      "당신은 질문-답변(Question-Answer) Task를 수행하는 AI 어시스턴트입니다.\n",
      "검색된 문맥(context)를 사용하여 질문(question)에 답하세요.\n",
      "만약, 문맥(context)으로부터 답을 찾을 수 없다면 '모른다'고 말하세요.\n",
      "한국어로 대답하세요.\n",
      "\n",
      "# Question: {question}\n",
      "# Context: {context}\n",
      "\"\"\"\n",
      "\n",
      "# 템플릿으로부터 채팅 프롬프트를 생성합니다.\n",
      "prompt = ChatPromptTemplate.from_template(template)\n",
      "\n",
      "# ChatOpenAI 모델을 초기화합니다.\n",
      "model = ChatOpenAI()\n",
      "\n",
      "# 검색 체인을 구성합니다.\n",
      "retrieval_chain = (\n",
      "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
      "    | prompt\n",
      "    | model\n",
      "    | StrOutputParser()\n",
      ")\n",
      "\n",
      "# 검색 체인을 실행하여 질문에 대한 답변을 얻습니다.\n",
      "response = retrieval_chain.invoke({\"question\": \"대한민국의 수도는 어디인가요?\", \"context\": \"서울은 대한민국의 수도입니다.\"})\n",
      "print(response)\n",
      "```\n",
      "\n",
      "이 예제에서는 지시사항을 포함한 템플릿을 사용하여 모델이 질문에 대한 답변을 생성하도록 설정하고 있습니다.\n",
      "\n",
      "### 출처\n",
      "- [LangChain Hub](https://wikidocs.net/233349)\n",
      "- [LangChain Prompts](https://wikidocs.net/233347)"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"question\": \"조금전에 이야기한 1번에 대해 설명해줘!\"}, config={\"configurable\": {\"session_id\": \"foo\"}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
