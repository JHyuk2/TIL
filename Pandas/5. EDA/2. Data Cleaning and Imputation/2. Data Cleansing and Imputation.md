# 2. Data Cleansing and Imputation



## 1) Addressing missing data

- Why is missing data a problem?
  - Less representative of the population
  - Can result in drawing incorrect conclusions

​	

### Count the number of missing values

```python
df.isna().sum()
```



### Strategies for addressing missing data

- Drop missing values

  - 5% or less of total values ~ 관측값이 전체 값의 5% 이하인 경우, 관측값을 제거

  ```python
  # 누락된 값 계산을 위한 임계값 설정.
  threshold = len(salaries) * 0.05
  
  # 임계값보다 낮은 수준의 missing value를 갖고 있는 애들은 drop해도 괜찮음.
  cols_to_drop = salaries.columns[salaries.isna().sum() <= threshold]
  salaries.dropna(subset=cols_to_drop, inplace=True)
  ```

  > dropna에서 subset 파라미터를 기억하자.

  

- Impute mean, median, mode ~ 보간법

  - Depends on distribution and context

  ```python
  # Drop이 가능하지 않은 missing value cols에 대해 적용할 수 있다.
  cols_with_missing_values = salaries.columns[salaries.isna().sum() > 0]
  
  # mode(최빈값)로 imputation
  # 전체 cols에 대해 슬라이싱(왜 마지막 값은 빼는지 잘 모르겠다.)
  for col in cols_with_missing_values[:-1]:
      # 각 cols를 mode로 보간해주는데, mode()[0] 과 같이 인덱싱을 해주어야 한다.
      ## 그렇지 않고 그냥 mode()로 하는 경우 series를 반환하기 때문.
      salaries[col].fillna(salaries[col].mode()[0])
  ```

  - mean은 이상치로부터 영향을 많이 받는다. 따라서 boxplot을 확인해보고, 값이 지나치게 차이가 많이 나면 median을 사용하는 게 좋다.

    

- Impute by sub-group

  - Different experience levels have different median salary

  ```python
  # 중앙값 구하기
  salaries_dict = salaries.groupby('Experience')['Salary_USD'].median().to_dict()
  # to_dict는 생략해도 된다.
  
  # Experience별 중앙값으로 치환하기.
  salaries['Salary_USD'] = salaries['Salary_USD'].fillna(salaries['Experience'].map(salaries_dict))
  ```



## 2) Converting and analyzing categorical data

숫자가 아닌 데이터를 필터링하려면 select_dtypes('object') 메서드를 사용할 수 있다.

```python
# object 타입을 먼저 골라낸 후,
salaries.select_dtypes('object').head()

# 값이 얼마나 들어있나 확인하기
salaries["Designation"].value_counts()

# 각각의 고유값은 몇 개 있을까?
len(salaries["Designation"].value_counts())
salaries["Designation"].nunique()
```



- 시각화 알록달록하게 하기

```python
from itertools import cycle, islice

my_colors = list(islice(cycle(['b', 'r', 'g', 'y', 'k']), 5,))

# custom color 생성 및 그라데이션
my_colors_gb = ['g', 'b'] * 5 # <-- 단순히 5개씩 반복
# 마찬가지로 두 개의 커스텀 색상 반복
my_colors_float = [(0.5, 0.4, 0.5), (0.75, 0.75, 0.25)]*5 
# 빠른 그라데이션 컬러 만들기
my_colors_grad = [(x/7.5, x/10.0, 0.75) for x in range(5)] 
```



### Categorical value control

- contains 함수와 np.select를 사용하여 카테고리를 나눠줄 수 있다.

```python
import numpy as np

# 모든 scientist에 대해 체크
scientist_filter = salaries['Designation'].str.contains('Scientist')
ml_filter = salaries['Designation'].str.contains('Machine Learning|AI') # | 를 추가해서 or로 만들어줌.
data_filter = salaries['Designation'].str.contains('^Data') 


# Category 생성
job_categories = ["Data Science", "Data Analytics",
                  "Data Engineering", "Machine Learning",
                  "Managerial", "Consultant"]

data_science = "Data Scientist|NLP"
data_analyst = "Analyst|Analytics"
data_engineer = "Data Engineer|ETL|Architect|Infrastructure"
ml_engineer = "Machine Learning|ML|Big Data|AI"
manager = "Manager|Head|Director|Lead|Principal|Staff"
consultant = "Consultant|Freelance"

# conditions 생성
conditions = [
    (salaries["Designation"].str.contains(data_science)),
    (salaries["Designation"].str.contains(data_analyst)),
    (salaries["Designation"].str.contains(data_engineer)),
    (salaries["Designation"].str.contains(ml_engineer)),
    (salaries["Designation"].str.contains(manager)),
    (salaries["Designation"].str.contains(consultant)),
]


## conditions에 따라 job_categories 맵핑
salaries["Job_Category"] = np.select(conditions,
                                     job_categories,
                                     default="Other")
```



## 3) Working with numeric data

