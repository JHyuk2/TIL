{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 라이브러리 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 설명\n",
    "\n",
    "- 오디오 처리: numpy, librosa, soundfile, pydub\n",
    "- 모델 실행: torch, transformers, accelerate\n",
    "- 응용 프로그램 구성: langchain, sentence-transformers\n",
    "- 데이터셋: datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설치한 라이브러리들 설명  \n",
    "\n",
    "### 핵심 AI 및 NLP 관련 라이브러리\n",
    "1. `torch`  \n",
    "PyTorch는 딥러닝 모델을 구축하고 훈련하는 데 사용하는 오픈소스 라이브러리입니다.  \n",
    "GPU 가속을 지원하며, 특히 자연어 처리와 컴퓨터 비전에서 널리 사용됩니다.  \n",
    "Transformers 모델을 실행하거나 커스텀 모델을 훈련할 때 사용됩니다.  \n",
    "\n",
    "2. `transformers`  \n",
    "Hugging Face에서 제공하는 라이브러리로, 사전 학습된 NLP 모델(BERT, GPT, Whisper 등)을 쉽게 사용할 수 있습니다.  \n",
    "Whisper 모델도 이 라이브러리를 통해 로드하여 사용합니다.  \n",
    "Whisper 모델로 음성 데이터를 텍스트로 변환하는 데 사용됩니다.  \n",
    "\n",
    "3. `accelerate`  \n",
    "Hugging Face에서 제공하는 라이브러리로, 모델 훈련과 추론을 가속화하고 여러 디바이스(CPU, GPU, TPU 등)를 활용하도록 지원합니다.  \n",
    "Whisper나 기타 Transformer 기반 모델을 실행 시 성능 최적화에 사용됩니다.  \n",
    "\n",
    "4. `langchain`  \n",
    "대규모 언어 모델(LLM)을 활용한 애플리케이션을 구축하는 프레임워크입니다.  \n",
    "여러 NLP 작업(질의응답, 대화 생성 등)을 연결하는 워크플로우를 구성할 때 사용됩니다.  \n",
    "LLM 기반 애플리케이션 개발에 사용됩니다. Whisper와 결합하여 음성 인식 후 처리 로직을 작성할 때 유용할 수 있습니다.  \n",
    "\n",
    "1. `sentence-transformers`  \n",
    "문장 수준의 임베딩(벡터 표현)을 생성하는 데 사용되는 라이브러리입니다.  \n",
    "텍스트 데이터의 유사도 측정이나 검색 작업에 널리 사용됩니다.  \n",
    "Whisper로 변환된 텍스트 데이터를 처리하거나 분석하는 데 사용될 수 있습니다.  \n",
    "\n",
    "\n",
    "### 오디오 처리 및 데이터 관련 라이브러리  \n",
    "1. `numpy==1.23.4`  \n",
    "Python의 대표적인 수치 계산 라이브러리로, 행렬 연산 및 고성능 배열 처리를 지원합니다.  \n",
    "Whisper 모델 및 오디오 데이터 처리에서 핵심적인 역할을 합니다.  \n",
    "PCM 데이터를 처리하거나 librosa와 함께 오디오 데이터 배열을 조작할 때 사용됩니다.  \n",
    "(`librosa`와 호환하기 위해 버전을 지정해주었습니다. 2.0.0 버전도 가능)  \n",
    "\n",
    "2. `librosa`  \n",
    "오디오 분석과 신호 처리를 위한 라이브러리입니다.  \n",
    "오디오 데이터를 주파수 영역(Mel Spectrogram)으로 변환하는 등 Whisper 모델과 직접적으로 연관이 있습니다.  \n",
    "Whisper 모델에 입력으로 제공되는 데이터를 전처리하거나 변환하는 데 사용됩니다.  \n",
    "\n",
    "3. `soundfile`  \n",
    "오디오 파일을 읽고 쓰는 데 사용되는 라이브러리입니다.  \n",
    "`librosa`가 내부적으로 의존합니다.  \n",
    "오디오 데이터를 로드하거나 저장할 때 사용됩니다.  \n",
    "\n",
    "4. `pydub`  \n",
    "오디오 데이터를 자르거나 합치는 등의 작업을 지원하는 라이브러리입니다.  \n",
    "ffmpeg와 함께 동작하며, Whisper에 입력으로 제공할 오디오를 준비하는 데 유용합니다.  \n",
    "긴 오디오 데이터를 30초 단위로 분할하거나 특정 포맷으로 변환할 때 사용됩니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers accelerate langchain sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.23.4 librosa soundfile pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '0d38672e0bbdbdc460af55b8bb84a15b2730db2819f2af64f9c777d4d586f2de',\n",
       "  'array': array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00024414, 0.00048828,\n",
       "         0.0005188 ]),\n",
       "  'sampling_rate': 16000}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. 데이터셋 활용\n",
    "- datasets / distil-whisper/librispeech_long\n",
    "  - 허깅페이스에서 제공하는 음성 데이터셋으로, 주로 ASR평가를 위해 사용\n",
    "  - 데이터 샘플\n",
    "  \n",
    "  ```python\n",
    "  {\n",
    "    \"audio\": {\n",
    "        \"path\": \"파일 경로 또는 파일 ID\",\n",
    "        \"array\": array([...]),  # PCM 데이터 배열\n",
    "        \"sampling_rate\": 16000  # 샘플링 속도\n",
    "    }\n",
    "  }\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[0]['audio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. 데이터셋 변환 (PCM -> pydub.AudioSegment)\n",
    "- PCM(`Pulse Code Modulation`): 디지털 오디오 데이터의 기본 형식  \n",
    "\n",
    "- **특징**\n",
    "  - 1. 비압축 데이터\n",
    "    - 압축 코덱(MP3, AAC)과 달리 압축되지 않은 원시의 데이터\n",
    "  - 2. 샘플링\n",
    "    - 아날로그 오디오 신호를 일정한 간격으로 측정하여 디지털 값으로 변환\n",
    "  - 3. 샘플 폭\n",
    "    - PCM이 차지하는 비트 수를 나타내는데, 16비트, 8비트 같은 게 있음.\n",
    "  - 4. 채널\n",
    "    - 오디오가 모노(1채널)인지, 스테레오(2채널)인지 결정.\n",
    "\n",
    "- **장단점**\n",
    "  - 장점\n",
    "    - 1. 고품질 데이터\n",
    "      - 압축되지 않은 원본 데이터를 제공하므로, 품질 손실이 없음\n",
    "    - 2. 표준화\n",
    "      - 오디오 처리 라이브러리나 딥러닝 모델에서 기본적으로 사용 가능\n",
    "  - 단점\n",
    "    - 1. 큰 파일 크기\n",
    "      - 압축되지 않았기에 크기가 큼\n",
    "    - 2. 직접 사용에 불편\n",
    "      - 사람이 바로 이해하거나 사용 불가능한 형태로, 추가 처리가 필요함.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "샘플 데이터를 짧게 보면\n",
    "\n",
    "```python\n",
    "{\n",
    "    'audio':{\n",
    "        'path': \"파일 경로 또는 파일 ID\",\n",
    "        'array' : [0.0023, 0.0035, -0.0046, ...], #PCM 데이터 배열\n",
    "        'sampling_rate' : 16000, # 샘플링 속도 (16,000Hz)\n",
    "        'channels' : 1, # 모노(1채널)\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '0d38672e0bbdbdc460af55b8bb84a15b2730db2819f2af64f9c777d4d586f2de',\n",
       " 'array': array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00024414, 0.00048828,\n",
       "        0.0005188 ]),\n",
       " 'sampling_rate': 16000}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_0.wav 저장 완료\n",
      "chunk_1.wav 저장 완료\n",
      "chunk_2.wav 저장 완료\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# 오디오 파일 로드\n",
    "\n",
    "# 오디오 데이터 추출 및 AudioSegment 변환\n",
    "audio_array = sample['array']\n",
    "sampling_rate = sample['sampling_rate']\n",
    "\n",
    "# PCM 데이터를 16비트 정수로 변환\n",
    "audio_data = np.array(audio_array * 32767, dtype=np.int16)\n",
    "\n",
    "# AudioSegment로 변환\n",
    "audio = AudioSegment(\n",
    "    audio_data.tobytes(),\n",
    "    frame_rate=sampling_rate,\n",
    "    sample_width=audio_data.dtype.itemsize,\n",
    "    channels=1\n",
    ")\n",
    "\n",
    "# 오디오 분할 (30초 단위)\n",
    "chunk_length_ms = 30 * 1000  # 30초\n",
    "chunks = [audio[i:i + chunk_length_ms] for i in range(0, len(audio), chunk_length_ms)]\n",
    "\n",
    "# 각 분할 저장\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.export(f\"chunk_{i}.wav\", format=\"wav\")\n",
    "    print(f\"chunk_{i}.wav 저장 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pydub.audio_segment.AudioSegment at 0x17dd5b4ddc0>,\n",
       " <pydub.audio_segment.AudioSegment at 0x17dd5b4de80>,\n",
       " <pydub.audio_segment.AudioSegment at 0x17db02edb50>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 불러오기 및 파이프라인 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.3` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from transformers.models.whisper import EncoderDecoderCache\n",
    "\n",
    "# CUDA 사용\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Whisper 모델 로드\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch_dtype,\n",
    "    # low_cpu_mem_usage=True, \n",
    "    use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# PCM 데이터를 Mel Spectorgram으로 변환 후 진행\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-large-v3-turbo\") # PCM -> Mel Spectrogram 입력 변환을 위함\n",
    "\n",
    "input_features = processor(\n",
    "    audio_array,\n",
    "    sampling_rate=sampling_rate,\n",
    "    return_tensors=\"pt\",\n",
    ").input_features\n",
    "\n",
    "\n",
    "# 신규 버전 업데이트) 기존 튜플을 EncoderDecoderCache로 변환\n",
    "# past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n",
    "attention_mask = torch.ones_like(input_features)  # 모든 입력이 활성화된 상태로 설정\n",
    "\n",
    "generates_ids = model.generate(\n",
    "    input_features,\n",
    "    temperature=0.7, # 다양성\n",
    "    num_beams=2, # beam search를 사용한 텍스트 생성\n",
    "    length_penalty=1.3, # 기본값 1, 긴 텍스트에 조금 더 유리하도록 설정\n",
    "    attention_mask=attention_mask, # 입력 데이터에 대해 명시적으로 attention_mask 생성 후 전달\n",
    ")\n",
    "transcription = processor.batch_decode(generates_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: [\" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind.\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"Transcription:\", transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 녹음한 파일로 ASR 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\dm705\\\\Study\\\\TIL\\\\Projects\\\\AI 영어대화'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 존재 여부: True\n"
     ]
    }
   ],
   "source": [
    "file_name = 'self_motivation.m4a'\n",
    "file_path = os.path.join(os.getcwd(), file_name)\n",
    "print(\"파일 존재 여부:\", os.path.exists(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAV 변환 성공\n"
     ]
    }
   ],
   "source": [
    "# 이미 만들어진 processor, model을 사용\n",
    "\n",
    "'''\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch_dtype,\n",
    "    # low_cpu_mem_usage=True, \n",
    "    use_safetensors=True\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-large-v3-turbo\") # PCM -> Mel Spectrogram 입력 변환을 위함\n",
    "'''\n",
    "\n",
    "# Whisper모델은 PCM데이터 or WAV형식 데이터를 필요로 하기 때문에, 변환해줌.\n",
    "import traceback # 전체 오류 스택 트레이스\n",
    "\n",
    "# load file\n",
    "try:\n",
    "    record_file = AudioSegment.from_file(file_path, format=\"m4a\")\n",
    "    # convert to WAV format\n",
    "    record_file.export(\"self_motivation.wav\", format='wav')\n",
    "    print(\"WAV 변환 성공\")\n",
    "except Exception as e:\n",
    "    print(\"오디오 변환 실패:\", e)\n",
    "    traceback.print_exc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FileNotFoundError가 발생하는 경우  \n",
    "> traceback.print_exc() 를 사용해서, 전체 오류 스택 트레이스를 출력할 수 있다.    \n",
    "> 나의 경우 `pydub`이 내부적으로 사용하는 `FFmpeg`가 설치되지 않아서 오류가 발생하는 것 같았음.  \n",
    "> - `Couldn't find ffprobe or avprobe` warning이 발생했기 때문.  \n",
    "> 때문에 `FFmpeg`를 아래와 같이 설치해줌.\n",
    "\n",
    "---\n",
    "#### 1. FFmpeg 다운로드\n",
    "- [FFmpeg 공식 웹사이트 접속](https://ffmpeg.org/download.html)\n",
    "- Windows 빌드 섹션에서 \"Windows builds by gyan.dev\" 링크 클릭.\n",
    "#### 2. Windows 빌드 다운로드\n",
    "- `ffmpeg-git-full.7z` 파일 다운로드\n",
    "#### 3. 압축 해제 후 환경변수 설정\n",
    "- 압축한 폴더의 `ffmpeg-bin` 폴더로 가서, 환경변수(시스템변수)로 추가.\n",
    "- bash창을 다시 닫고 아래의 명령어 실행\n",
    "- ```bash\n",
    "    ffmpeg -version  \n",
    "    ffprobe -version\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAV 변환 성공\n"
     ]
    }
   ],
   "source": [
    "# 재도전\n",
    "\n",
    "# load file\n",
    "try:\n",
    "    record_file = AudioSegment.from_file(file_path, format=\"m4a\")\n",
    "    # convert to WAV format\n",
    "    record_file.export(\"self_motivation.wav\", format='wav')\n",
    "    print(\"WAV 변환 성공\")\n",
    "except Exception as e:\n",
    "    print(\"오디오 변환 실패:\", e)\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오디오 데이터 길이: 492203\n",
      "샘플링 속도: 16000 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dm705\\AppData\\Local\\Temp\\ipykernel_15856\\1433415770.py:2: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sampling_rate = librosa.load(file_path, sr=16000)\n",
      "c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    }
   ],
   "source": [
    "# librosa를 이용한 파일 읽기\n",
    "audio, sampling_rate = librosa.load(file_path, sr=16000)\n",
    "\n",
    "print(f'오디오 데이터 길이: {len(audio)}')\n",
    "print(f'샘플링 속도: {sampling_rate} Hz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1) 기본 파라미터로 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:   어떤 작업이든 지금부터 25분 동안 집중을 하고 5분 동안 휴식을 갖겠습니다. 앞으로 뽀모도로 기법을 활용해서 나의 하루를 바꿔나갈 것입니다. 나는 천재다. 나는 할 수 있다. 나는 좋은 일이 많이 생긴다. 나는 내가 원하는 모든 일을 이룰 수 있는 힘을 갖고 있다. 나는 준비가 되었고 나는 실행할 힘이 있다.\n"
     ]
    }
   ],
   "source": [
    "input_features = processor(\n",
    "    audio,\n",
    "    sampling_rate=16000,\n",
    "    return_tensors='pt'\n",
    ").input_features\n",
    "\n",
    "generates_ids = model.generate(input_features)\n",
    "transcription = processor.batch_decode(generates_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"Transcription: \", transcription[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) 하이퍼파라미터 튜닝 및 어텐션 마스크 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.3` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:   어떤 작업이든 지금부터 25분 동안 집중을 하고 5분 동안 휴식을 갖겠습니다. 앞으로 뽀모도로 기법을 활용해서 나의 하루를 바꿔나갈 것입니다. 나는 천재다. 나는 할 수 있다. 나는 좋은 일이 많이 생긴다. 나는 내가 원하는 모든 일을 이룰 수 있는 힘을 갖고 있다. 나는 준비가 되었고, 나는 실행할 힘이 있다.\n"
     ]
    }
   ],
   "source": [
    "attention_mask = torch.ones_like(input_features)  # 모든 입력이 활성화된 상태로 설정\n",
    "\n",
    "generates_ids = model.generate(\n",
    "    input_features,\n",
    "    temperature=0.7, # 다양성\n",
    "    length_penalty=1.3, # 기본값 1, 긴 텍스트에 조금 더 유리하도록 설정\n",
    "    attention_mask=attention_mask, # 입력 데이터에 대해 명시적으로 attention_mask 생성 후 전달\n",
    ")\n",
    "transcription = processor.batch_decode(generates_ids, skip_special_tokens=True)\n",
    "print(\"Transcription: \", transcription[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "# .env 파일에서 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "# Hugging Face 토큰 가져오기\n",
    "hf_token = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "\n",
    "# Hugging Face Access Token 입력\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'bartowski/gemma-2-9b-it-GGUF'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bartowski/gemma-2-9b-it-GGUF' is the correct path to a directory containing all relevant files for a GemmaTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# bnb_config = BitsAndBytesConfig(\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#     _load_in_4bit=True,\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#     # quant_method='bnb'\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m     15\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbartowski/gemma-2-9b-it-GGUF\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 16\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m llm \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     18\u001b[0m     pretrained_model_name_or_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTheBloke/Llama-2-9B-GGUF\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m     model_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-2-9b-it-Q3_K_L.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     20\u001b[0m     model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# gpu_layers=50\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# model = AutoModelForCausalLM.from_pretrained(\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#     pretrained_model_name_or_path=model_name,\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#     device_map='auto',\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 테스트 입력\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:940\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    937\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 940\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    942\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2016\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2013\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[0;32m   2014\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[0;32m   2015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[1;32m-> 2016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2019\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2021\u001b[0m     )\n\u001b[0;32m   2023\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'bartowski/gemma-2-9b-it-GGUF'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bartowski/gemma-2-9b-it-GGUF' is the correct path to a directory containing all relevant files for a GemmaTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import BitsAndBytesConfig\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# 4-bit 양자화된 모델 로드\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     _load_in_4bit=True,\n",
    "#     # quant_method='bnb'\n",
    "# )\n",
    "\n",
    "model_name = \"bartowski/gemma-2-9b-it-GGUF\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"TheBloke/Llama-2-9B-GGUF\",\n",
    "    model_file=\"gemma-2-9b-it-Q3_K_L.gguf\", \n",
    "    model_type=\"llama\", \n",
    "    # gpu_layers=50\n",
    "    )\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     pretrained_model_name_or_path=model_name,\n",
    "#     device_map='auto',\n",
    "#     )\n",
    "\n",
    "# 테스트 입력\n",
    "# inputs = tokenizer(transcription, return_tensors=\"pt\")\n",
    "# outputs = model.generate(inputs[\"input_ids\"], max_length=510, num_return_sequences=1)\n",
    "\n",
    "# 결과 확인\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not load model bartowski/gemma-2-9b-it-GGUF with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 3929, in from_pretrained\n    raise EnvironmentError(\nOSError: bartowski/gemma-2-9b-it-GGUF does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbartowski/gemma-2-9b-it-GGUF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemma-2-9b-it-Q3_K_L.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:940\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    939\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 940\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    950\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    951\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\pipelines\\base.py:302\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    301\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    303\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         )\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model bartowski/gemma-2-9b-it-GGUF with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 3929, in from_pretrained\n    raise EnvironmentError(\nOSError: bartowski/gemma-2-9b-it-GGUF does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"bartowski/gemma-2-9b-it-GGUF\",\n",
    "    model_name=\"gemma-2-9b-it-Q3_K_L.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())  # True여야 GPU가 활성화된 상태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.20.1+cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "torchvision.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
