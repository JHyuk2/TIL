{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ëª…\n",
    "\n",
    "- ì˜¤ë””ì˜¤ ì²˜ë¦¬: numpy, librosa, soundfile, pydub\n",
    "- ëª¨ë¸ ì‹¤í–‰: torch, transformers, accelerate\n",
    "- ì‘ìš© í”„ë¡œê·¸ë¨ êµ¬ì„±: langchain, sentence-transformers\n",
    "- ë°ì´í„°ì…‹: datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì„¤ì¹˜í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ ì„¤ëª…  \n",
    "\n",
    "### í•µì‹¬ AI ë° NLP ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "1. `torch`  \n",
    "PyTorchëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  í›ˆë ¨í•˜ëŠ” ë° ì‚¬ìš©í•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.  \n",
    "GPU ê°€ì†ì„ ì§€ì›í•˜ë©°, íŠ¹íˆ ìì—°ì–´ ì²˜ë¦¬ì™€ ì»´í“¨í„° ë¹„ì „ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
    "Transformers ëª¨ë¸ì„ ì‹¤í–‰í•˜ê±°ë‚˜ ì»¤ìŠ¤í…€ ëª¨ë¸ì„ í›ˆë ¨í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
    "\n",
    "2. `transformers`  \n",
    "Hugging Faceì—ì„œ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ì‚¬ì „ í•™ìŠµëœ NLP ëª¨ë¸(BERT, GPT, Whisper ë“±)ì„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "Whisper ëª¨ë¸ë„ ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.  \n",
    "Whisper ëª¨ë¸ë¡œ ìŒì„± ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
    "\n",
    "3. `accelerate`  \n",
    "Hugging Faceì—ì„œ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ëª¨ë¸ í›ˆë ¨ê³¼ ì¶”ë¡ ì„ ê°€ì†í™”í•˜ê³  ì—¬ëŸ¬ ë””ë°”ì´ìŠ¤(CPU, GPU, TPU ë“±)ë¥¼ í™œìš©í•˜ë„ë¡ ì§€ì›í•©ë‹ˆë‹¤.  \n",
    "Whisperë‚˜ ê¸°íƒ€ Transformer ê¸°ë°˜ ëª¨ë¸ì„ ì‹¤í–‰ ì‹œ ì„±ëŠ¥ ìµœì í™”ì— ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
    "\n",
    "4. `langchain`  \n",
    "ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.  \n",
    "ì—¬ëŸ¬ NLP ì‘ì—…(ì§ˆì˜ì‘ë‹µ, ëŒ€í™” ìƒì„± ë“±)ì„ ì—°ê²°í•˜ëŠ” ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì„±í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
    "LLM ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì— ì‚¬ìš©ë©ë‹ˆë‹¤. Whisperì™€ ê²°í•©í•˜ì—¬ ìŒì„± ì¸ì‹ í›„ ì²˜ë¦¬ ë¡œì§ì„ ì‘ì„±í•  ë•Œ ìœ ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "\n",
    "1. `sentence-transformers`  \n",
    "ë¬¸ì¥ ìˆ˜ì¤€ì˜ ì„ë² ë”©(ë²¡í„° í‘œí˜„)ì„ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.  \n",
    "í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ìœ ì‚¬ë„ ì¸¡ì •ì´ë‚˜ ê²€ìƒ‰ ì‘ì—…ì— ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
    "Whisperë¡œ ë³€í™˜ëœ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê±°ë‚˜ ë¶„ì„í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "\n",
    "\n",
    "### ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë° ë°ì´í„° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬  \n",
    "1. `numpy==1.23.4`  \n",
    "Pythonì˜ ëŒ€í‘œì ì¸ ìˆ˜ì¹˜ ê³„ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, í–‰ë ¬ ì—°ì‚° ë° ê³ ì„±ëŠ¥ ë°°ì—´ ì²˜ë¦¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.  \n",
    "Whisper ëª¨ë¸ ë° ì˜¤ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬ì—ì„œ í•µì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.  \n",
    "PCM ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê±°ë‚˜ librosaì™€ í•¨ê»˜ ì˜¤ë””ì˜¤ ë°ì´í„° ë°°ì—´ì„ ì¡°ì‘í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
    "(`librosa`ì™€ í˜¸í™˜í•˜ê¸° ìœ„í•´ ë²„ì „ì„ ì§€ì •í•´ì£¼ì—ˆìŠµë‹ˆë‹¤. 2.0.0 ë²„ì „ë„ ê°€ëŠ¥)  \n",
    "\n",
    "2. `librosa`  \n",
    "ì˜¤ë””ì˜¤ ë¶„ì„ê³¼ ì‹ í˜¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.  \n",
    "ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ì£¼íŒŒìˆ˜ ì˜ì—­(Mel Spectrogram)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë“± Whisper ëª¨ë¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ì—°ê´€ì´ ìˆìŠµë‹ˆë‹¤.  \n",
    "Whisper ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ì œê³µë˜ëŠ” ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê±°ë‚˜ ë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
    "\n",
    "3. `soundfile`  \n",
    "ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì½ê³  ì“°ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.  \n",
    "`librosa`ê°€ ë‚´ë¶€ì ìœ¼ë¡œ ì˜ì¡´í•©ë‹ˆë‹¤.  \n",
    "ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê±°ë‚˜ ì €ì¥í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
    "\n",
    "4. `pydub`  \n",
    "ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ìë¥´ê±°ë‚˜ í•©ì¹˜ëŠ” ë“±ì˜ ì‘ì—…ì„ ì§€ì›í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.  \n",
    "ffmpegì™€ í•¨ê»˜ ë™ì‘í•˜ë©°, Whisperì— ì…ë ¥ìœ¼ë¡œ ì œê³µí•  ì˜¤ë””ì˜¤ë¥¼ ì¤€ë¹„í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.  \n",
    "ê¸´ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ 30ì´ˆ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê±°ë‚˜ íŠ¹ì • í¬ë§·ìœ¼ë¡œ ë³€í™˜í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers accelerate langchain sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.23.4 librosa soundfile pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ASR êµ¬í˜„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Ipython Notebookì—ì„œ ë…¹ìŒ ë²„íŠ¼ UI êµ¬í˜„í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import threading\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë…¹ìŒ ê´€ë ¨ ë³€ìˆ˜\n",
    "device_id = 1 # í™•ì¸ëœ ë§ˆì´í¬ ì¥ì¹˜ ID\n",
    "recording = False\n",
    "paused = False\n",
    "audio_data = []\n",
    "sample_rate = 16000 # Whisper ê¶Œì¥ ìƒ˜í”Œë§ ë ˆì´íŠ¸\n",
    "sample_number = 1\n",
    "elapsed_time = 0 # ê²½ê³¼ ì‹œê°„ (ì´ˆ)\n",
    "timer_running = False # íƒ€ì´ë¨¸ ì‹¤í–‰ ì—¬ë¶€\n",
    "\n",
    "# ë…¹ìŒ í•¨ìˆ˜\n",
    "def record_audio():\n",
    "    global recording, paused, audio_data\n",
    "    audio_data = []\n",
    "\n",
    "    with sd.InputStream(samplerate=sample_rate, channels=1, dtype='int16', device=device_id) as stream:\n",
    "        while recording:\n",
    "            if not paused:\n",
    "                frame, _ = stream.read(1024)\n",
    "                audio_data.append(frame)\n",
    "\n",
    "# íƒ€ì´ë¨¸ í•¨ìˆ˜\n",
    "def update_timer():\n",
    "    global elapsed_time, timer_running\n",
    "    timer_running = True\n",
    "    while timer_running:\n",
    "        time.sleep(1)\n",
    "        if recording and not paused:\n",
    "            elapsed_time += 1\n",
    "            timer_label.value = f\"â³ ê²½ê³¼ ì‹œê°„: {elapsed_time}ì´ˆ\"\n",
    "\n",
    "# ë…¹ìŒ ì‹œì‘ í•¨ìˆ˜\n",
    "def start_recording(_):\n",
    "    global recording, paused, elapsed_time, timer_running\n",
    "    if not recording:\n",
    "        recording=True\n",
    "        paused = False\n",
    "        elapsed_time = 0 # íƒ€ì´ë¨¸ ì´ˆê¸°í™”\n",
    "        threading.Thread(target=record_audio, daemon=True).start()\n",
    "        threading.Thread(target=update_timer, daemon=True).start()\n",
    "        status_label.value = \"ğŸ™ë…¹ìŒ ì¤‘...\"\n",
    "        timer_label.value = \"ê²½ê³¼ ì‹œê°„: 0ì´ˆ\"\n",
    "\n",
    "# ì¼ì‹œì •ì§€ í•¨ìˆ˜\n",
    "def pause_recording(_):\n",
    "    global paused\n",
    "    if recording:\n",
    "        paused = not paused\n",
    "        status_label.value = \"â¸ ë…¹ìŒ ì¼ì‹œì •ì§€ë¨...\" if paused else \"ğŸ™ ë…¹ìŒ ì¬ê°œë¨...\"\n",
    "\n",
    "\n",
    "# ë…¹ìŒ ì¤‘ì§€ í•¨ìˆ˜\n",
    "def stop_recording(_):\n",
    "    global recording, timer_running\n",
    "    if recording:\n",
    "        recording = False\n",
    "        timer_running = False\n",
    "        status_label.value = \"âœ… ë…¹ìŒ ì™„ë£Œ!\"\n",
    "        save_audio()\n",
    "        timer_label.value = \"ê²½ê³¼ ì‹œê°„: 0ì´ˆ\"\n",
    "\n",
    "\n",
    "# ë…¹ìŒëœ íŒŒì¼ ì €ì¥ í•¨ìˆ˜\n",
    "def save_audio():\n",
    "    global sample_number\n",
    "    if len(audio_data) > 0:\n",
    "        audio_array = np.concatenate(audio_data, axis=0)\n",
    "        audio_name = f\"recorded_audio_{sample_number}.wav\"\n",
    "        sf.write(audio_name, audio_array, sample_rate)\n",
    "        sample_number += 1\n",
    "        status_label.value = f\"ğŸ¤ ë…¹ìŒ íŒŒì¼ ì €ì¥ë¨: {audio_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b03a8d3eda4528886536386846475b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='ë…¹ìŒ ì‹œì‘', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a4f068b0114f9cafa0ee880c90209e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='warning', description='ì¼ì‹œì •ì§€', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4bab95308c4e3a93acf76e614e2b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='danger', description='ë…¹ìŒ ì¢…ë£Œ', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e253fb7d2ee94f4fa6c08eaea3c1a60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='ê²½ê³¼ ì‹œê°„: 0ì´ˆ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42c0f31a5a446419de494afaa837470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# UI ë²„íŠ¼ ìƒì„±\n",
    "start_button = widgets.Button(description=\"ë…¹ìŒ ì‹œì‘\", button_style=\"success\")\n",
    "pause_button = widgets.Button(description=\"ì¼ì‹œì •ì§€\", button_style=\"warning\")\n",
    "stop_button = widgets.Button(description=\"ë…¹ìŒ ì¢…ë£Œ\", button_style=\"danger\")\n",
    "status_label = widgets.Label(value=\"\")\n",
    "timer_label = widgets.Label(value=\"ê²½ê³¼ ì‹œê°„: 0ì´ˆ\")\n",
    "\n",
    "# ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸ ì—°ê²°\n",
    "start_button.on_click(start_recording)\n",
    "pause_button.on_click(pause_recording)\n",
    "stop_button.on_click(stop_recording)\n",
    "\n",
    "# UI í‘œì‹œ\n",
    "display(start_button, pause_button, stop_button, timer_label, status_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. ìŒì„±íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŒì„± ê¸¸ì´: 90.05ì´ˆ\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "# ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ\n",
    "audio_name = 'recorded_audio_3.wav'\n",
    "audio_array, sr = librosa.load(audio_name, sr=16000) # Whisper 16kHz ìƒ˜í”Œë§\n",
    "print(f'ìŒì„± ê¸¸ì´: {audio_array.shape[0] / 16000 :.2f}ì´ˆ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 ë…¹ìŒíŒŒì¼ 30ì´ˆì”© ì²­í‚¹í•˜ê¸°\n",
    "ê¸°ë³¸ì ìœ¼ë¡œ Whisperëª¨ë¸ì€ 30ì´ˆì”© ì˜ë¼ì„œ ì²˜ë¦¬í•˜ëŠ” ê²Œ ê°€ì¥ ì•ˆì •ì ì´ê¸°ì—,  \n",
    "30ì´ˆ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³ , ì´ë¥¼ ì—°ê²°í•˜ì—¬ transcriptionì„ ë‚´ë†“ëŠ” ê²Œ ì¼ë°˜ì ì¸ ë°©ë²•."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_0.wav ì €ì¥ ì™„ë£Œ\n",
      "chunk_1.wav ì €ì¥ ì™„ë£Œ\n",
      "chunk_2.wav ì €ì¥ ì™„ë£Œ\n",
      "chunk_3.wav ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# PCM ë°ì´í„°ë¥¼ 16ë¹„íŠ¸ ì •ìˆ˜ë¡œ ë³€í™˜\n",
    "audio_data = np.array(audio_array * 32767, dtype=np.int16)\n",
    "\n",
    "# AudioSegmentë¡œ ë³€í™˜\n",
    "audio = AudioSegment(\n",
    "    audio_data.tobytes(),\n",
    "    frame_rate=16000,\n",
    "    sample_width=audio_data.dtype.itemsize,\n",
    "    channels=1\n",
    ")\n",
    "\n",
    "# ì˜¤ë””ì˜¤ ë¶„í•  (30ì´ˆ ë‹¨ìœ„)\n",
    "chunk_length_ms = 30 * 1000  # 30ì´ˆ\n",
    "chunks = [audio[i:i + chunk_length_ms] for i in range(0, len(audio), chunk_length_ms)]\n",
    "\n",
    "# ê° ë¶„í•  ì €ì¥\n",
    "chunk_audio_list = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_audio = f\"chunk_{i}.wav\"\n",
    "    chunk.export(chunk_audio, format=\"wav\")\n",
    "    print(f\"{chunk_audio} ì €ì¥ ì™„ë£Œ\")\n",
    "    chunk_audio_list.append(chunk_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ë° íŒŒì´í”„ë¼ì¸ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "c:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.3` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "import torch\n",
    "# from transformers.models.whisper import EncoderDecoderCache\n",
    "\n",
    "# CUDA ì‚¬ìš©\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Whisper ëª¨ë¸ ë¡œë“œ\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch_dtype,\n",
    "    # low_cpu_mem_usage=True, \n",
    "    use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# PCM ë°ì´í„°ë¥¼ Mel Spectorgramìœ¼ë¡œ ë³€í™˜ í›„ ì§„í–‰\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-large-v3-turbo\") # PCM -> Mel Spectrogram ì…ë ¥ ë³€í™˜ì„ ìœ„í•¨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ASR_generation(audio_array):\n",
    "    input_features = processor(\n",
    "        audio_array,\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_features\n",
    "\n",
    "\n",
    "    # ì‹ ê·œ ë²„ì „ ì—…ë°ì´íŠ¸) ê¸°ì¡´ íŠœí”Œì„ EncoderDecoderCacheë¡œ ë³€í™˜\n",
    "    # past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n",
    "    attention_mask = torch.ones_like(input_features)  # ëª¨ë“  ì…ë ¥ì´ í™œì„±í™”ëœ ìƒíƒœë¡œ ì„¤ì •\n",
    "\n",
    "    generates_ids = model.generate(\n",
    "        input_features,\n",
    "        # max_length=300,\n",
    "        temperature=0.1, # ë‹¤ì–‘ì„±\n",
    "        num_beams=2, # beam searchë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "        length_penalty=1.3, # ê¸°ë³¸ê°’ 1, ê¸´ í…ìŠ¤íŠ¸ì— ì¡°ê¸ˆ ë” ìœ ë¦¬í•˜ë„ë¡ ì„¤ì •\n",
    "        attention_mask=attention_mask, # ì…ë ¥ ë°ì´í„°ì— ëŒ€í•´ ëª…ì‹œì ìœ¼ë¡œ attention_mask ìƒì„± í›„ ì „ë‹¬\n",
    "    )\n",
    "\n",
    "    transcription = processor.batch_decode(generates_ids, skip_special_tokens=True)\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.3` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "transcriptions = []\n",
    "# ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ\n",
    "for chunk_audio in chunk_audio_list:\n",
    "    chunk_audio_array, sr = librosa.load(chunk_audio, sr=16000)\n",
    "    temp_script = ASR_generation(chunk_audio_array)\n",
    "    transcriptions.append(temp_script[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ì´ìµì„ ì–¼ë§ˆë‚˜ ëƒˆëŠ”ì§€, êµ¬ë…ìê°€ ì–¼ë§ˆë‚˜ ëŠ˜ì—ˆëŠ”ì§€, ë‚´ì¶œì´ ì–´ë–»ê²Œ ëëŠ”ì§€, íŒ”ë¡œì›Œê°€ ëŠ˜ì—ˆëŠ”ì§€, ì•„ë‹ˆë©´ ë‚´ê°€ ë­˜ ë°°ì› ëŠ”ì§€ê¹Œì§€ ë‹¤ ë¦¬ì¡°íŠ¸ì— ë“¤ì–´ê°€ìš”. ê·¸ë˜ì„œ ì´ê²Œ SDAI ìŠ¤íƒ€ì¼ë§¨ì†Œë“œì´ê³ , ì—¬ê¸°ì„œ ì œê°€ ì•„ê¹Œ ë§ì”€ë“œë ¸ë˜, ì·¨ì—…ì„ ì˜í•˜ëŠ” í•™ìƒë“¤ì˜ í•™êµ ì°¨ì´ëŠ”, ê·¸ë˜ì„œ ì´ ê²½í—˜ìœ¼ë¡œ ë‚´ê°€ ì´ íšŒì‚¬ì˜ ì´ ë¹„ì •í™”ì™€ ì–´ë–»ê²Œ ë§¤ì¹˜ì‹œí‚¬ ê±´ì§€ê¹Œì§€ ì–˜ê¸°í•©ë‹ˆë‹¤.',\n",
       " ' ì•„, ë©´ì ‘ì„ ë³´ë‹¤ ë³´ë©´ ì´ ì •ë„ë¡œ ì–˜ê¸°ë¥¼ í•˜ë©´ ì‚¬ì‹¤ì€ ì•„ê¹Œ STARì„ ì–˜ê¸°í•˜ëŠ” ë™ì•ˆ ìŸ¤ëŠ” ì–´ë–¤ ì• êµ¬ë‚˜ ì•Œê²Œ ë˜ëŠ” ê²ƒì¸ë° ë§ˆì§€ë§‰ì— ë‚˜ì—ê²Œ ë‹¤ í–ˆìœ¼ë‹ˆê¹Œ ë‹¹ì‹ ì—ì„œ ë‚´ê°€ ì•Œê³  ìˆì–´ìš”. ë§ì•„ ë–¨ì–´ì§€ëŠ” ê±°ì§€ ì•„ëŠ” ë‘ ê°€ì§€ê°€ ë§ì•„ ë–¨ì–´ì§€ë©´ ì§„ì§œ ì•„ëŠ” ê±°ì§€ ì—¬ê¸°ì„œ ë˜ ì¤‘ìš”í•œ ê²Œ í•˜ë‚˜ ë” ìˆëŠ”ë° ë§ˆì¸ë“œì¦ˆì‹œê±°ë“ ìš”. ë§ˆì¸ë“œì¦ˆì‹œ ë­ëƒë©´ ì—¬ê¸° ì•„ë‹ˆì–´ë„ ë¼.',\n",
       " ' ì•„, ì €ì˜ ê¸°ê´€ì€?',\n",
       " ' you']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ì´ìµì„ ì–¼ë§ˆë‚˜ ëƒˆëŠ”ì§€, êµ¬ë…ìê°€ ì–¼ë§ˆë‚˜ ëŠ˜ì—ˆëŠ”ì§€, ë‚´ì¶œì´ ì–´ë–»ê²Œ ëëŠ”ì§€, íŒ”ë¡œì›Œê°€ ëŠ˜ì—ˆëŠ”ì§€, ì•„ë‹ˆë©´ ë‚´ê°€ ë­˜ ë°°ì› ëŠ”ì§€ê¹Œì§€ ë‹¤ ë¦¬ì¡°íŠ¸ì— ë“¤ì–´ê°€ìš”. ê·¸ë˜ì„œ ì´ê²Œ SDAI ìŠ¤íƒ€ì¼ë§¨ì†Œë“œì´ê³ , ì—¬ê¸°ì„œ ì œê°€ ì•„ê¹Œ ë§ì”€ë“œë ¸ë˜, ì·¨ì—…ì„ ì˜í•˜ëŠ” í•™ìƒë“¤ì˜ í•™êµ ì°¨ì´ëŠ”, ê·¸ë˜ì„œ ì´ ê²½í—˜ìœ¼ë¡œ ë‚´ê°€ ì´ íšŒì‚¬ì˜ ì´ ë¹„ì •í™”ì™€ ì–´ë–»ê²Œ ë§¤ì¹˜ì‹œí‚¬ ê±´ì§€ê¹Œì§€ ì–˜ê¸°í•©ë‹ˆë‹¤. ì•„, ë©´ì ‘ì„ ë³´ë‹¤ ë³´ë©´ ì´ ì •ë„ë¡œ ì–˜ê¸°ë¥¼ í•˜ë©´ ì‚¬ì‹¤ì€ ì•„ê¹Œ STARì„ ì–˜ê¸°í•˜ëŠ” ë™ì•ˆ ìŸ¤ëŠ” ì–´ë–¤ ì• êµ¬ë‚˜ ì•Œê²Œ ë˜ëŠ” ê²ƒì¸ë° ë§ˆì§€ë§‰ì— ë‚˜ì—ê²Œ ë‹¤ í–ˆìœ¼ë‹ˆê¹Œ ë‹¹ì‹ ì—ì„œ ë‚´ê°€ ì•Œê³  ìˆì–´ìš”. ë§ì•„ ë–¨ì–´ì§€ëŠ” ê±°ì§€ ì•„ëŠ” ë‘ ê°€ì§€ê°€ ë§ì•„ ë–¨ì–´ì§€ë©´ ì§„ì§œ ì•„ëŠ” ê±°ì§€ ì—¬ê¸°ì„œ ë˜ ì¤‘ìš”í•œ ê²Œ í•˜ë‚˜ ë” ìˆëŠ”ë° ë§ˆì¸ë“œì¦ˆì‹œê±°ë“ ìš”. ë§ˆì¸ë“œì¦ˆì‹œ ë­ëƒë©´ ì—¬ê¸° ì•„ë‹ˆì–´ë„ ë¼. ì•„, ì €ì˜ ê¸°ê´€ì€? you\n"
     ]
    }
   ],
   "source": [
    "transcription = ''.join(transcriptions)\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 64.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m transcriptions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[1;32m----> 3\u001b[0m     script \u001b[38;5;241m=\u001b[39m \u001b[43mASR_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     transcriptions\u001b[38;5;241m.\u001b[39mappend(script)\n\u001b[0;32m      6\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(transcriptions)\n",
      "Cell \u001b[1;32mIn[54], line 2\u001b[0m, in \u001b[0;36mASR_generation\u001b[1;34m(audio_array)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mASR_generation\u001b[39m(audio_array):\n\u001b[1;32m----> 2\u001b[0m     input_features \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43maudio_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minput_features\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# ì‹ ê·œ ë²„ì „ ì—…ë°ì´íŠ¸) ê¸°ì¡´ íŠœí”Œì„ EncoderDecoderCacheë¡œ ë³€í™˜\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(input_features)  \u001b[38;5;66;03m# ëª¨ë“  ì…ë ¥ì´ í™œì„±í™”ëœ ìƒíƒœë¡œ ì„¤ì •\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12\\Lib\\site-packages\\transformers\\models\\whisper\\processing_whisper.py:69\u001b[0m, in \u001b[0;36mWhisperProcessor.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to specify either an `audio` or `text` input to process.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m audio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 69\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12\\Lib\\site-packages\\transformers\\models\\whisper\\feature_extraction_whisper.py:270\u001b[0m, in \u001b[0;36mWhisperFeatureExtractor.__call__\u001b[1;34m(self, raw_speech, truncation, pad_to_multiple_of, return_tensors, return_attention_mask, padding, max_length, sampling_rate, do_normalize, device, return_token_timestamps, **kwargs)\u001b[0m\n\u001b[0;32m    268\u001b[0m     raw_speech \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39masarray([speech], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mT \u001b[38;5;28;01mfor\u001b[39;00m speech \u001b[38;5;129;01min\u001b[39;00m raw_speech]\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_speech, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m--> 270\u001b[0m     raw_speech \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_speech\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_speech, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m raw_speech\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdtype(np\u001b[38;5;241m.\u001b[39mfloat64):\n\u001b[0;32m    272\u001b[0m     raw_speech \u001b[38;5;241m=\u001b[39m raw_speech\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 64."
     ]
    }
   ],
   "source": [
    "# transcriptions = []\n",
    "# for chunk in chunks:\n",
    "#     script = ASR_generation(chunk)\n",
    "#     transcriptions.append(script)\n",
    "\n",
    "# result = ' '.join(transcriptions)\n",
    "# print(len(result))\n",
    "# print(f\"Transcription: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ë…¹ìŒí•œ íŒŒì¼ë¡œ ASR í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\win\\\\Hyuk2Coding\\\\TIL\\\\Projects\\\\AI ì˜ì–´ëŒ€í™”'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: False\n"
     ]
    }
   ],
   "source": [
    "file_name = 'self_motivation.m4a'\n",
    "file_path = os.path.join(os.getcwd(), file_name)\n",
    "print(\"íŒŒì¼ ì¡´ì¬ ì—¬ë¶€:\", os.path.exists(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë””ì˜¤ ë³€í™˜ ì‹¤íŒ¨: name 'AudioSegment' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\win\\AppData\\Local\\Temp\\ipykernel_21596\\1809049952.py\", line 20, in <module>\n",
      "    record_file = AudioSegment.from_file(file_path, format=\"m4a\")\n",
      "                  ^^^^^^^^^^^^\n",
      "NameError: name 'AudioSegment' is not defined\n"
     ]
    }
   ],
   "source": [
    "# ì´ë¯¸ ë§Œë“¤ì–´ì§„ processor, modelì„ ì‚¬ìš©\n",
    "\n",
    "'''\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch_dtype,\n",
    "    # low_cpu_mem_usage=True, \n",
    "    use_safetensors=True\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-large-v3-turbo\") # PCM -> Mel Spectrogram ì…ë ¥ ë³€í™˜ì„ ìœ„í•¨\n",
    "'''\n",
    "\n",
    "# Whisperëª¨ë¸ì€ PCMë°ì´í„° or WAVí˜•ì‹ ë°ì´í„°ë¥¼ í•„ìš”ë¡œ í•˜ê¸° ë•Œë¬¸ì—, ë³€í™˜í•´ì¤Œ.\n",
    "import traceback # ì „ì²´ ì˜¤ë¥˜ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤\n",
    "\n",
    "# load file\n",
    "try:\n",
    "    record_file = AudioSegment.from_file(file_path, format=\"m4a\")\n",
    "    # convert to WAV format\n",
    "    record_file.export(\"self_motivation.wav\", format='wav')\n",
    "    print(\"WAV ë³€í™˜ ì„±ê³µ\")\n",
    "except Exception as e:\n",
    "    print(\"ì˜¤ë””ì˜¤ ë³€í™˜ ì‹¤íŒ¨:\", e)\n",
    "    traceback.print_exc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FileNotFoundErrorê°€ ë°œìƒí•˜ëŠ” ê²½ìš°  \n",
    "> traceback.print_exc() ë¥¼ ì‚¬ìš©í•´ì„œ, ì „ì²´ ì˜¤ë¥˜ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ë¥¼ ì¶œë ¥í•  ìˆ˜ ìˆë‹¤.    \n",
    "> ë‚˜ì˜ ê²½ìš° `pydub`ì´ ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” `FFmpeg`ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ëŠ” ê²ƒ ê°™ì•˜ìŒ.  \n",
    "> - `Couldn't find ffprobe or avprobe` warningì´ ë°œìƒí–ˆê¸° ë•Œë¬¸.  \n",
    "> ë•Œë¬¸ì— `FFmpeg`ë¥¼ ì•„ë˜ì™€ ê°™ì´ ì„¤ì¹˜í•´ì¤Œ.\n",
    "\n",
    "---\n",
    "#### 1. FFmpeg ë‹¤ìš´ë¡œë“œ\n",
    "- [FFmpeg ê³µì‹ ì›¹ì‚¬ì´íŠ¸ ì ‘ì†](https://ffmpeg.org/download.html)\n",
    "- Windows ë¹Œë“œ ì„¹ì…˜ì—ì„œ \"Windows builds by gyan.dev\" ë§í¬ í´ë¦­.\n",
    "#### 2. Windows ë¹Œë“œ ë‹¤ìš´ë¡œë“œ\n",
    "- `ffmpeg-git-full.7z` íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
    "#### 3. ì••ì¶• í•´ì œ í›„ í™˜ê²½ë³€ìˆ˜ ì„¤ì •\n",
    "- ì••ì¶•í•œ í´ë”ì˜ `ffmpeg-bin` í´ë”ë¡œ ê°€ì„œ, í™˜ê²½ë³€ìˆ˜(ì‹œìŠ¤í…œë³€ìˆ˜)ë¡œ ì¶”ê°€.\n",
    "- bashì°½ì„ ë‹¤ì‹œ ë‹«ê³  ì•„ë˜ì˜ ëª…ë ¹ì–´ ì‹¤í–‰\n",
    "- ```bash\n",
    "    ffmpeg -version  \n",
    "    ffprobe -version\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë””ì˜¤ ë³€í™˜ ì‹¤íŒ¨: name 'AudioSegment' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\win\\AppData\\Local\\Temp\\ipykernel_21596\\1067670247.py\", line 5, in <module>\n",
      "    record_file = AudioSegment.from_file(file_path, format=\"m4a\")\n",
      "                  ^^^^^^^^^^^^\n",
      "NameError: name 'AudioSegment' is not defined\n"
     ]
    }
   ],
   "source": [
    "# ì¬ë„ì „\n",
    "\n",
    "# load file\n",
    "try:\n",
    "    record_file = AudioSegment.from_file(file_path, format=\"m4a\")\n",
    "    # convert to WAV format\n",
    "    record_file.export(\"self_motivation.wav\", format='wav')\n",
    "    print(\"WAV ë³€í™˜ ì„±ê³µ\")\n",
    "except Exception as e:\n",
    "    print(\"ì˜¤ë””ì˜¤ ë³€í™˜ ì‹¤íŒ¨:\", e)\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win\\AppData\\Local\\Temp\\ipykernel_21596\\1433415770.py:2: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sampling_rate = librosa.load(file_path, sr=16000)\n",
      "c:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\win\\\\Hyuk2Coding\\\\TIL\\\\Projects\\\\AI ì˜ì–´ëŒ€í™”\\\\self_motivation.m4a'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12\\Lib\\site-packages\\librosa\\core\\audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12\\Lib\\site-packages\\librosa\\core\\audio.py:209\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12\\Lib\\site-packages\\soundfile.py:690\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd, compression_level, bitrate_mode)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    689\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12\\Lib\\site-packages\\soundfile.py:1265\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1264\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[1;32m-> 1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[1;31mLibsndfileError\u001b[0m: Error opening 'c:\\\\Users\\\\win\\\\Hyuk2Coding\\\\TIL\\\\Projects\\\\AI ì˜ì–´ëŒ€í™”\\\\self_motivation.m4a': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# librosaë¥¼ ì´ìš©í•œ íŒŒì¼ ì½ê¸°\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m audio, sampling_rate \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì˜¤ë””ì˜¤ ë°ì´í„° ê¸¸ì´: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(audio)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mìƒ˜í”Œë§ ì†ë„: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msampling_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Hz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12\\Lib\\site-packages\\librosa\\core\\audio.py:184\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[0;32m    181\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12\\Lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12\\Lib\\site-packages\\librosa\\util\\decorators.py:59\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[0;32m     58\u001b[0m )\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12\\Lib\\site-packages\\librosa\\core\\audio.py:240\u001b[0m, in \u001b[0;36m__audioread_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    237\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[0;32m    243\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12\\Lib\\site-packages\\audioread\\__init__.py:127\u001b[0m, in \u001b[0;36maudio_open\u001b[1;34m(path, backends)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m BackendClass \u001b[38;5;129;01min\u001b[39;00m backends:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBackendClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError:\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12\\Lib\\site-packages\\audioread\\rawread.py:59\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m aifc\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\win\\\\Hyuk2Coding\\\\TIL\\\\Projects\\\\AI ì˜ì–´ëŒ€í™”\\\\self_motivation.m4a'"
     ]
    }
   ],
   "source": [
    "# librosaë¥¼ ì´ìš©í•œ íŒŒì¼ ì½ê¸°\n",
    "audio, sampling_rate = librosa.load(file_path, sr=16000)\n",
    "\n",
    "print(f'ì˜¤ë””ì˜¤ ë°ì´í„° ê¸¸ì´: {len(audio)}')\n",
    "print(f'ìƒ˜í”Œë§ ì†ë„: {sampling_rate} Hz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1) ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¡œ ì§„í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:   ì–´ë–¤ ì‘ì—…ì´ë“  ì§€ê¸ˆë¶€í„° 25ë¶„ ë™ì•ˆ ì§‘ì¤‘ì„ í•˜ê³  5ë¶„ ë™ì•ˆ íœ´ì‹ì„ ê°–ê² ìŠµë‹ˆë‹¤. ì•ìœ¼ë¡œ ë½€ëª¨ë„ë¡œ ê¸°ë²•ì„ í™œìš©í•´ì„œ ë‚˜ì˜ í•˜ë£¨ë¥¼ ë°”ê¿”ë‚˜ê°ˆ ê²ƒì…ë‹ˆë‹¤. ë‚˜ëŠ” ì²œì¬ë‹¤. ë‚˜ëŠ” í•  ìˆ˜ ìˆë‹¤. ë‚˜ëŠ” ì¢‹ì€ ì¼ì´ ë§ì´ ìƒê¸´ë‹¤. ë‚˜ëŠ” ë‚´ê°€ ì›í•˜ëŠ” ëª¨ë“  ì¼ì„ ì´ë£° ìˆ˜ ìˆëŠ” í˜ì„ ê°–ê³  ìˆë‹¤. ë‚˜ëŠ” ì¤€ë¹„ê°€ ë˜ì—ˆê³  ë‚˜ëŠ” ì‹¤í–‰í•  í˜ì´ ìˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "input_features = processor(\n",
    "    audio,\n",
    "    sampling_rate=16000,\n",
    "    return_tensors='pt'\n",
    ").input_features\n",
    "\n",
    "generates_ids = model.generate(input_features)\n",
    "transcription = processor.batch_decode(generates_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"Transcription: \", transcription[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë° ì–´í…ì…˜ ë§ˆìŠ¤í¬ ì œê³µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.3` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:   ì–´ë–¤ ì‘ì—…ì´ë“  ì§€ê¸ˆë¶€í„° 25ë¶„ ë™ì•ˆ ì§‘ì¤‘ì„ í•˜ê³  5ë¶„ ë™ì•ˆ íœ´ì‹ì„ ê°–ê² ìŠµë‹ˆë‹¤. ì•ìœ¼ë¡œ ë½€ëª¨ë„ë¡œ ê¸°ë²•ì„ í™œìš©í•´ì„œ ë‚˜ì˜ í•˜ë£¨ë¥¼ ë°”ê¿”ë‚˜ê°ˆ ê²ƒì…ë‹ˆë‹¤. ë‚˜ëŠ” ì²œì¬ë‹¤. ë‚˜ëŠ” í•  ìˆ˜ ìˆë‹¤. ë‚˜ëŠ” ì¢‹ì€ ì¼ì´ ë§ì´ ìƒê¸´ë‹¤. ë‚˜ëŠ” ë‚´ê°€ ì›í•˜ëŠ” ëª¨ë“  ì¼ì„ ì´ë£° ìˆ˜ ìˆëŠ” í˜ì„ ê°–ê³  ìˆë‹¤. ë‚˜ëŠ” ì¤€ë¹„ê°€ ë˜ì—ˆê³ , ë‚˜ëŠ” ì‹¤í–‰í•  í˜ì´ ìˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "attention_mask = torch.ones_like(input_features)  # ëª¨ë“  ì…ë ¥ì´ í™œì„±í™”ëœ ìƒíƒœë¡œ ì„¤ì •\n",
    "\n",
    "generates_ids = model.generate(\n",
    "    input_features,\n",
    "    temperature=0.7, # ë‹¤ì–‘ì„±\n",
    "    length_penalty=1.3, # ê¸°ë³¸ê°’ 1, ê¸´ í…ìŠ¤íŠ¸ì— ì¡°ê¸ˆ ë” ìœ ë¦¬í•˜ë„ë¡ ì„¤ì •\n",
    "    attention_mask=attention_mask, # ì…ë ¥ ë°ì´í„°ì— ëŒ€í•´ ëª…ì‹œì ìœ¼ë¡œ attention_mask ìƒì„± í›„ ì „ë‹¬\n",
    ")\n",
    "transcription = processor.batch_decode(generates_ids, skip_special_tokens=True)\n",
    "print(\"Transcription: \", transcription[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# Hugging Face í† í° ê°€ì ¸ì˜¤ê¸°\n",
    "hf_token = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "\n",
    "# Hugging Face Access Token ì…ë ¥\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'bartowski/gemma-2-9b-it-GGUF'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bartowski/gemma-2-9b-it-GGUF' is the correct path to a directory containing all relevant files for a GemmaTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# bnb_config = BitsAndBytesConfig(\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#     _load_in_4bit=True,\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#     # quant_method='bnb'\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m     15\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbartowski/gemma-2-9b-it-GGUF\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 16\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m llm \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     18\u001b[0m     pretrained_model_name_or_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTheBloke/Llama-2-9B-GGUF\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m     model_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-2-9b-it-Q3_K_L.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     20\u001b[0m     model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# gpu_layers=50\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# model = AutoModelForCausalLM.from_pretrained(\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#     pretrained_model_name_or_path=model_name,\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#     device_map='auto',\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# í…ŒìŠ¤íŠ¸ ì…ë ¥\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:940\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    937\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 940\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    942\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2016\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2013\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[0;32m   2014\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[0;32m   2015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[1;32m-> 2016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2019\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2021\u001b[0m     )\n\u001b[0;32m   2023\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'bartowski/gemma-2-9b-it-GGUF'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bartowski/gemma-2-9b-it-GGUF' is the correct path to a directory containing all relevant files for a GemmaTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import BitsAndBytesConfig\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# 4-bit ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     _load_in_4bit=True,\n",
    "#     # quant_method='bnb'\n",
    "# )\n",
    "\n",
    "model_name = \"bartowski/gemma-2-9b-it-GGUF\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"TheBloke/Llama-2-9B-GGUF\",\n",
    "    model_file=\"gemma-2-9b-it-Q3_K_L.gguf\", \n",
    "    model_type=\"llama\", \n",
    "    # gpu_layers=50\n",
    "    )\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     pretrained_model_name_or_path=model_name,\n",
    "#     device_map='auto',\n",
    "#     )\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì…ë ¥\n",
    "# inputs = tokenizer(transcription, return_tensors=\"pt\")\n",
    "# outputs = model.generate(inputs[\"input_ids\"], max_length=510, num_return_sequences=1)\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not load model bartowski/gemma-2-9b-it-GGUF with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 3929, in from_pretrained\n    raise EnvironmentError(\nOSError: bartowski/gemma-2-9b-it-GGUF does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbartowski/gemma-2-9b-it-GGUF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemma-2-9b-it-Q3_K_L.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:940\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    939\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 940\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    950\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    951\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\pipelines\\base.py:302\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    301\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    303\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         )\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model bartowski/gemma-2-9b-it-GGUF with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 3929, in from_pretrained\n    raise EnvironmentError(\nOSError: bartowski/gemma-2-9b-it-GGUF does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"bartowski/gemma-2-9b-it-GGUF\",\n",
    "    model_name=\"gemma-2-9b-it-Q3_K_L.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())  # Trueì—¬ì•¼ GPUê°€ í™œì„±í™”ëœ ìƒíƒœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.20.1+cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "torchvision.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
