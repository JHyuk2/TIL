{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 라이브러리 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 설명\n",
    "\n",
    "- 오디오 처리: numpy, librosa, soundfile, pydub\n",
    "- 모델 실행: torch, transformers, accelerate\n",
    "- 응용 프로그램 구성: langchain, sentence-transformers\n",
    "- 데이터셋: datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설치한 라이브러리들 설명  \n",
    "\n",
    "### 핵심 AI 및 NLP 관련 라이브러리\n",
    "1. `torch`  \n",
    "PyTorch는 딥러닝 모델을 구축하고 훈련하는 데 사용하는 오픈소스 라이브러리입니다.  \n",
    "GPU 가속을 지원하며, 특히 자연어 처리와 컴퓨터 비전에서 널리 사용됩니다.  \n",
    "Transformers 모델을 실행하거나 커스텀 모델을 훈련할 때 사용됩니다.  \n",
    "\n",
    "2. `transformers`  \n",
    "Hugging Face에서 제공하는 라이브러리로, 사전 학습된 NLP 모델(BERT, GPT, Whisper 등)을 쉽게 사용할 수 있습니다.  \n",
    "Whisper 모델도 이 라이브러리를 통해 로드하여 사용합니다.  \n",
    "Whisper 모델로 음성 데이터를 텍스트로 변환하는 데 사용됩니다.  \n",
    "\n",
    "3. `accelerate`  \n",
    "Hugging Face에서 제공하는 라이브러리로, 모델 훈련과 추론을 가속화하고 여러 디바이스(CPU, GPU, TPU 등)를 활용하도록 지원합니다.  \n",
    "Whisper나 기타 Transformer 기반 모델을 실행 시 성능 최적화에 사용됩니다.  \n",
    "\n",
    "4. `langchain`  \n",
    "대규모 언어 모델(LLM)을 활용한 애플리케이션을 구축하는 프레임워크입니다.  \n",
    "여러 NLP 작업(질의응답, 대화 생성 등)을 연결하는 워크플로우를 구성할 때 사용됩니다.  \n",
    "LLM 기반 애플리케이션 개발에 사용됩니다. Whisper와 결합하여 음성 인식 후 처리 로직을 작성할 때 유용할 수 있습니다.  \n",
    "\n",
    "1. `sentence-transformers`  \n",
    "문장 수준의 임베딩(벡터 표현)을 생성하는 데 사용되는 라이브러리입니다.  \n",
    "텍스트 데이터의 유사도 측정이나 검색 작업에 널리 사용됩니다.  \n",
    "Whisper로 변환된 텍스트 데이터를 처리하거나 분석하는 데 사용될 수 있습니다.  \n",
    "\n",
    "\n",
    "### 오디오 처리 및 데이터 관련 라이브러리  \n",
    "1. `numpy==1.23.4`  \n",
    "Python의 대표적인 수치 계산 라이브러리로, 행렬 연산 및 고성능 배열 처리를 지원합니다.  \n",
    "Whisper 모델 및 오디오 데이터 처리에서 핵심적인 역할을 합니다.  \n",
    "PCM 데이터를 처리하거나 librosa와 함께 오디오 데이터 배열을 조작할 때 사용됩니다.  \n",
    "(`librosa`와 호환하기 위해 버전을 지정해주었습니다. 2.0.0 버전도 가능)  \n",
    "\n",
    "2. `librosa`  \n",
    "오디오 분석과 신호 처리를 위한 라이브러리입니다.  \n",
    "오디오 데이터를 주파수 영역(Mel Spectrogram)으로 변환하는 등 Whisper 모델과 직접적으로 연관이 있습니다.  \n",
    "Whisper 모델에 입력으로 제공되는 데이터를 전처리하거나 변환하는 데 사용됩니다.  \n",
    "\n",
    "3. `soundfile`  \n",
    "오디오 파일을 읽고 쓰는 데 사용되는 라이브러리입니다.  \n",
    "`librosa`가 내부적으로 의존합니다.  \n",
    "오디오 데이터를 로드하거나 저장할 때 사용됩니다.  \n",
    "\n",
    "4. `pydub`  \n",
    "오디오 데이터를 자르거나 합치는 등의 작업을 지원하는 라이브러리입니다.  \n",
    "ffmpeg와 함께 동작하며, Whisper에 입력으로 제공할 오디오를 준비하는 데 유용합니다.  \n",
    "긴 오디오 데이터를 30초 단위로 분할하거나 특정 포맷으로 변환할 때 사용됩니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers accelerate langchain sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.23.4 librosa soundfile pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ASR 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Ipython Notebook에서 녹음 버튼 UI 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import threading\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 녹음 관련 변수\n",
    "device_id = 1 # 확인된 마이크 장치 ID\n",
    "recording = False\n",
    "paused = False\n",
    "audio_data = []\n",
    "sample_rate = 16000 # Whisper 권장 샘플링 레이트\n",
    "sample_number = 1\n",
    "elapsed_time = 0 # 경과 시간 (초)\n",
    "timer_running = False # 타이머 실행 여부\n",
    "\n",
    "# 녹음 함수\n",
    "def record_audio():\n",
    "    global recording, paused, audio_data\n",
    "    audio_data = []\n",
    "\n",
    "    with sd.InputStream(samplerate=sample_rate, channels=1, dtype='int16', device=device_id) as stream:\n",
    "        while recording:\n",
    "            if not paused:\n",
    "                frame, _ = stream.read(1024)\n",
    "                audio_data.append(frame)\n",
    "\n",
    "# 타이머 함수\n",
    "def update_timer():\n",
    "    global elapsed_time, timer_running\n",
    "    timer_running = True\n",
    "    while timer_running:\n",
    "        time.sleep(1)\n",
    "        if recording and not paused:\n",
    "            elapsed_time += 1\n",
    "            timer_label.value = f\"⏳ 경과 시간: {elapsed_time}초\"\n",
    "\n",
    "# 녹음 시작 함수\n",
    "def start_recording(_):\n",
    "    global recording, paused, elapsed_time, timer_running\n",
    "    if not recording:\n",
    "        recording=True\n",
    "        paused = False\n",
    "        elapsed_time = 0 # 타이머 초기화\n",
    "        threading.Thread(target=record_audio, daemon=True).start()\n",
    "        threading.Thread(target=update_timer, daemon=True).start()\n",
    "        status_label.value = \"🎙녹음 중...\"\n",
    "        timer_label.value = \"경과 시간: 0초\"\n",
    "\n",
    "# 일시정지 함수\n",
    "def pause_recording(_):\n",
    "    global paused\n",
    "    if recording:\n",
    "        paused = not paused\n",
    "        status_label.value = \"⏸ 녹음 일시정지됨...\" if paused else \"🎙 녹음 재개됨...\"\n",
    "\n",
    "\n",
    "# 녹음 중지 함수\n",
    "def stop_recording(_):\n",
    "    global recording, timer_running\n",
    "    if recording:\n",
    "        recording = False\n",
    "        timer_running = False\n",
    "        status_label.value = \"✅ 녹음 완료!\"\n",
    "        save_audio()\n",
    "        timer_label.value = \"경과 시간: 0초\"\n",
    "\n",
    "\n",
    "# 녹음된 파일 저장 함수\n",
    "def save_audio():\n",
    "    global sample_number\n",
    "    if len(audio_data) > 0:\n",
    "        audio_array = np.concatenate(audio_data, axis=0)\n",
    "        audio_name = f\"recorded_audio_{sample_number}.wav\"\n",
    "        sf.write(audio_name, audio_array, sample_rate)\n",
    "        sample_number += 1\n",
    "        status_label.value = f\"🎤 녹음 파일 저장됨: {audio_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b03a8d3eda4528886536386846475b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='녹음 시작', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a4f068b0114f9cafa0ee880c90209e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='warning', description='일시정지', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4bab95308c4e3a93acf76e614e2b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='danger', description='녹음 종료', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e253fb7d2ee94f4fa6c08eaea3c1a60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='경과 시간: 0초')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42c0f31a5a446419de494afaa837470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# UI 버튼 생성\n",
    "start_button = widgets.Button(description=\"녹음 시작\", button_style=\"success\")\n",
    "pause_button = widgets.Button(description=\"일시정지\", button_style=\"warning\")\n",
    "stop_button = widgets.Button(description=\"녹음 종료\", button_style=\"danger\")\n",
    "status_label = widgets.Label(value=\"\")\n",
    "timer_label = widgets.Label(value=\"경과 시간: 0초\")\n",
    "\n",
    "# 버튼 클릭 이벤트 연결\n",
    "start_button.on_click(start_recording)\n",
    "pause_button.on_click(pause_recording)\n",
    "stop_button.on_click(stop_recording)\n",
    "\n",
    "# UI 표시\n",
    "display(start_button, pause_button, stop_button, timer_label, status_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. 음성파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "음성 길이: 90.05초\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "# 오디오 파일 로드\n",
    "audio_name = 'recorded_audio_3.wav'\n",
    "audio_array, sr = librosa.load(audio_name, sr=16000) # Whisper 16kHz 샘플링\n",
    "print(f'음성 길이: {audio_array.shape[0] / 16000 :.2f}초')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 녹음파일 30초씩 청킹하기\n",
    "기본적으로 Whisper모델은 30초씩 잘라서 처리하는 게 가장 안정적이기에,  \n",
    "30초 단위로 나누고, 이를 연결하여 transcription을 내놓는 게 일반적인 방법."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_0.wav 저장 완료\n",
      "chunk_1.wav 저장 완료\n",
      "chunk_2.wav 저장 완료\n",
      "chunk_3.wav 저장 완료\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# PCM 데이터를 16비트 정수로 변환\n",
    "audio_data = np.array(audio_array * 32767, dtype=np.int16)\n",
    "\n",
    "# AudioSegment로 변환\n",
    "audio = AudioSegment(\n",
    "    audio_data.tobytes(),\n",
    "    frame_rate=16000,\n",
    "    sample_width=audio_data.dtype.itemsize,\n",
    "    channels=1\n",
    ")\n",
    "\n",
    "# 오디오 분할 (30초 단위)\n",
    "chunk_length_ms = 30 * 1000  # 30초\n",
    "chunks = [audio[i:i + chunk_length_ms] for i in range(0, len(audio), chunk_length_ms)]\n",
    "\n",
    "# 각 분할 저장\n",
    "chunk_audio_list = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_audio = f\"chunk_{i}.wav\"\n",
    "    chunk.export(chunk_audio, format=\"wav\")\n",
    "    print(f\"{chunk_audio} 저장 완료\")\n",
    "    chunk_audio_list.append(chunk_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. 모델 불러오기 및 파이프라인 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "c:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-HAZ4-uhy-py3.12\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.3` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "import torch\n",
    "# from transformers.models.whisper import EncoderDecoderCache\n",
    "\n",
    "# CUDA 사용\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Whisper 모델 로드\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch_dtype,\n",
    "    # low_cpu_mem_usage=True, \n",
    "    use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# PCM 데이터를 Mel Spectorgram으로 변환 후 진행\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-large-v3-turbo\") # PCM -> Mel Spectrogram 입력 변환을 위함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ASR_generation(audio_array):\n",
    "    input_features = processor(\n",
    "        audio_array,\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_features\n",
    "\n",
    "\n",
    "    # 신규 버전 업데이트) 기존 튜플을 EncoderDecoderCache로 변환\n",
    "    # past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n",
    "    attention_mask = torch.ones_like(input_features)  # 모든 입력이 활성화된 상태로 설정\n",
    "\n",
    "    generates_ids = model.generate(\n",
    "        input_features,\n",
    "        # max_length=300,\n",
    "        temperature=0.1, # 다양성\n",
    "        num_beams=2, # beam search를 사용한 텍스트 생성\n",
    "        length_penalty=1.3, # 기본값 1, 긴 텍스트에 조금 더 유리하도록 설정\n",
    "        attention_mask=attention_mask, # 입력 데이터에 대해 명시적으로 attention_mask 생성 후 전달\n",
    "    )\n",
    "\n",
    "    transcription = processor.batch_decode(generates_ids, skip_special_tokens=True)\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-HAZ4-uhy-py3.12\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.3` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "transcriptions = []\n",
    "# 오디오 파일 로드\n",
    "for chunk_audio in chunk_audio_list:\n",
    "    chunk_audio_array, sr = librosa.load(chunk_audio, sr=16000)\n",
    "    temp_script = ASR_generation(chunk_audio_array)\n",
    "    transcriptions.append(temp_script[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 이익을 얼마나 냈는지, 구독자가 얼마나 늘었는지, 내출이 어떻게 됐는지, 팔로워가 늘었는지, 아니면 내가 뭘 배웠는지까지 다 리조트에 들어가요. 그래서 이게 SDAI 스타일맨소드이고, 여기서 제가 아까 말씀드렸던, 취업을 잘하는 학생들의 학교 차이는, 그래서 이 경험으로 내가 이 회사의 이 비정화와 어떻게 매치시킬 건지까지 얘기합니다.',\n",
       " ' 아, 면접을 보다 보면 이 정도로 얘기를 하면 사실은 아까 STAR을 얘기하는 동안 쟤는 어떤 애구나 알게 되는 것인데 마지막에 나에게 다 했으니까 당신에서 내가 알고 있어요. 맞아 떨어지는 거지 아는 두 가지가 맞아 떨어지면 진짜 아는 거지 여기서 또 중요한 게 하나 더 있는데 마인드즈시거든요. 마인드즈시 뭐냐면 여기 아니어도 돼.',\n",
       " ' 아, 저의 기관은?',\n",
       " ' you']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 이익을 얼마나 냈는지, 구독자가 얼마나 늘었는지, 내출이 어떻게 됐는지, 팔로워가 늘었는지, 아니면 내가 뭘 배웠는지까지 다 리조트에 들어가요. 그래서 이게 SDAI 스타일맨소드이고, 여기서 제가 아까 말씀드렸던, 취업을 잘하는 학생들의 학교 차이는, 그래서 이 경험으로 내가 이 회사의 이 비정화와 어떻게 매치시킬 건지까지 얘기합니다. 아, 면접을 보다 보면 이 정도로 얘기를 하면 사실은 아까 STAR을 얘기하는 동안 쟤는 어떤 애구나 알게 되는 것인데 마지막에 나에게 다 했으니까 당신에서 내가 알고 있어요. 맞아 떨어지는 거지 아는 두 가지가 맞아 떨어지면 진짜 아는 거지 여기서 또 중요한 게 하나 더 있는데 마인드즈시거든요. 마인드즈시 뭐냐면 여기 아니어도 돼. 아, 저의 기관은? you\n"
     ]
    }
   ],
   "source": [
    "transcription = ''.join(transcriptions)\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 64.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m transcriptions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[1;32m----> 3\u001b[0m     script \u001b[38;5;241m=\u001b[39m \u001b[43mASR_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     transcriptions\u001b[38;5;241m.\u001b[39mappend(script)\n\u001b[0;32m      6\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(transcriptions)\n",
      "Cell \u001b[1;32mIn[54], line 2\u001b[0m, in \u001b[0;36mASR_generation\u001b[1;34m(audio_array)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mASR_generation\u001b[39m(audio_array):\n\u001b[1;32m----> 2\u001b[0m     input_features \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43maudio_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minput_features\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# 신규 버전 업데이트) 기존 튜플을 EncoderDecoderCache로 변환\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(input_features)  \u001b[38;5;66;03m# 모든 입력이 활성화된 상태로 설정\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-HAZ4-uhy-py3.12\\Lib\\site-packages\\transformers\\models\\whisper\\processing_whisper.py:69\u001b[0m, in \u001b[0;36mWhisperProcessor.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to specify either an `audio` or `text` input to process.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m audio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 69\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-HAZ4-uhy-py3.12\\Lib\\site-packages\\transformers\\models\\whisper\\feature_extraction_whisper.py:270\u001b[0m, in \u001b[0;36mWhisperFeatureExtractor.__call__\u001b[1;34m(self, raw_speech, truncation, pad_to_multiple_of, return_tensors, return_attention_mask, padding, max_length, sampling_rate, do_normalize, device, return_token_timestamps, **kwargs)\u001b[0m\n\u001b[0;32m    268\u001b[0m     raw_speech \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39masarray([speech], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mT \u001b[38;5;28;01mfor\u001b[39;00m speech \u001b[38;5;129;01min\u001b[39;00m raw_speech]\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_speech, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m--> 270\u001b[0m     raw_speech \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_speech\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_speech, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m raw_speech\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdtype(np\u001b[38;5;241m.\u001b[39mfloat64):\n\u001b[0;32m    272\u001b[0m     raw_speech \u001b[38;5;241m=\u001b[39m raw_speech\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 64."
     ]
    }
   ],
   "source": [
    "# transcriptions = []\n",
    "# for chunk in chunks:\n",
    "#     script = ASR_generation(chunk)\n",
    "#     transcriptions.append(script)\n",
    "\n",
    "# result = ' '.join(transcriptions)\n",
    "# print(len(result))\n",
    "# print(f\"Transcription: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 녹음한 파일로 ASR 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\win\\\\Hyuk2Coding\\\\TIL\\\\Projects\\\\AI 영어대화'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 존재 여부: False\n"
     ]
    }
   ],
   "source": [
    "file_name = 'self_motivation.m4a'\n",
    "file_path = os.path.join(os.getcwd(), file_name)\n",
    "print(\"파일 존재 여부:\", os.path.exists(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오디오 변환 실패: name 'AudioSegment' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\win\\AppData\\Local\\Temp\\ipykernel_21596\\1809049952.py\", line 20, in <module>\n",
      "    record_file = AudioSegment.from_file(file_path, format=\"m4a\")\n",
      "                  ^^^^^^^^^^^^\n",
      "NameError: name 'AudioSegment' is not defined\n"
     ]
    }
   ],
   "source": [
    "# 이미 만들어진 processor, model을 사용\n",
    "\n",
    "'''\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch_dtype,\n",
    "    # low_cpu_mem_usage=True, \n",
    "    use_safetensors=True\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-large-v3-turbo\") # PCM -> Mel Spectrogram 입력 변환을 위함\n",
    "'''\n",
    "\n",
    "# Whisper모델은 PCM데이터 or WAV형식 데이터를 필요로 하기 때문에, 변환해줌.\n",
    "import traceback # 전체 오류 스택 트레이스\n",
    "\n",
    "# load file\n",
    "try:\n",
    "    record_file = AudioSegment.from_file(file_path, format=\"m4a\")\n",
    "    # convert to WAV format\n",
    "    record_file.export(\"self_motivation.wav\", format='wav')\n",
    "    print(\"WAV 변환 성공\")\n",
    "except Exception as e:\n",
    "    print(\"오디오 변환 실패:\", e)\n",
    "    traceback.print_exc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FileNotFoundError가 발생하는 경우  \n",
    "> traceback.print_exc() 를 사용해서, 전체 오류 스택 트레이스를 출력할 수 있다.    \n",
    "> 나의 경우 `pydub`이 내부적으로 사용하는 `FFmpeg`가 설치되지 않아서 오류가 발생하는 것 같았음.  \n",
    "> - `Couldn't find ffprobe or avprobe` warning이 발생했기 때문.  \n",
    "> 때문에 `FFmpeg`를 아래와 같이 설치해줌.\n",
    "\n",
    "---\n",
    "#### 1. FFmpeg 다운로드\n",
    "- [FFmpeg 공식 웹사이트 접속](https://ffmpeg.org/download.html)\n",
    "- Windows 빌드 섹션에서 \"Windows builds by gyan.dev\" 링크 클릭.\n",
    "#### 2. Windows 빌드 다운로드\n",
    "- `ffmpeg-git-full.7z` 파일 다운로드\n",
    "#### 3. 압축 해제 후 환경변수 설정\n",
    "- 압축한 폴더의 `ffmpeg-bin` 폴더로 가서, 환경변수(시스템변수)로 추가.\n",
    "- bash창을 다시 닫고 아래의 명령어 실행\n",
    "- ```bash\n",
    "    ffmpeg -version  \n",
    "    ffprobe -version\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오디오 변환 실패: name 'AudioSegment' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\win\\AppData\\Local\\Temp\\ipykernel_21596\\1067670247.py\", line 5, in <module>\n",
      "    record_file = AudioSegment.from_file(file_path, format=\"m4a\")\n",
      "                  ^^^^^^^^^^^^\n",
      "NameError: name 'AudioSegment' is not defined\n"
     ]
    }
   ],
   "source": [
    "# 재도전\n",
    "\n",
    "# load file\n",
    "try:\n",
    "    record_file = AudioSegment.from_file(file_path, format=\"m4a\")\n",
    "    # convert to WAV format\n",
    "    record_file.export(\"self_motivation.wav\", format='wav')\n",
    "    print(\"WAV 변환 성공\")\n",
    "except Exception as e:\n",
    "    print(\"오디오 변환 실패:\", e)\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win\\AppData\\Local\\Temp\\ipykernel_21596\\1433415770.py:2: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sampling_rate = librosa.load(file_path, sr=16000)\n",
      "c:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-HAZ4-uhy-py3.12\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\win\\\\Hyuk2Coding\\\\TIL\\\\Projects\\\\AI 영어대화\\\\self_motivation.m4a'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-HAZ4-uhy-py3.12\\Lib\\site-packages\\librosa\\core\\audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-HAZ4-uhy-py3.12\\Lib\\site-packages\\librosa\\core\\audio.py:209\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-HAZ4-uhy-py3.12\\Lib\\site-packages\\soundfile.py:690\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd, compression_level, bitrate_mode)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    689\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-HAZ4-uhy-py3.12\\Lib\\site-packages\\soundfile.py:1265\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1264\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[1;32m-> 1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[1;31mLibsndfileError\u001b[0m: Error opening 'c:\\\\Users\\\\win\\\\Hyuk2Coding\\\\TIL\\\\Projects\\\\AI 영어대화\\\\self_motivation.m4a': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# librosa를 이용한 파일 읽기\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m audio, sampling_rate \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m오디오 데이터 길이: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(audio)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m샘플링 속도: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msampling_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Hz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-HAZ4-uhy-py3.12\\Lib\\site-packages\\librosa\\core\\audio.py:184\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[0;32m    181\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-HAZ4-uhy-py3.12\\Lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-HAZ4-uhy-py3.12\\Lib\\site-packages\\librosa\\util\\decorators.py:59\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[0;32m     58\u001b[0m )\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-HAZ4-uhy-py3.12\\Lib\\site-packages\\librosa\\core\\audio.py:240\u001b[0m, in \u001b[0;36m__audioread_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    237\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[0;32m    243\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-HAZ4-uhy-py3.12\\Lib\\site-packages\\audioread\\__init__.py:127\u001b[0m, in \u001b[0;36maudio_open\u001b[1;34m(path, backends)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m BackendClass \u001b[38;5;129;01min\u001b[39;00m backends:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBackendClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError:\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-HAZ4-uhy-py3.12\\Lib\\site-packages\\audioread\\rawread.py:59\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m aifc\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\win\\\\Hyuk2Coding\\\\TIL\\\\Projects\\\\AI 영어대화\\\\self_motivation.m4a'"
     ]
    }
   ],
   "source": [
    "# librosa를 이용한 파일 읽기\n",
    "audio, sampling_rate = librosa.load(file_path, sr=16000)\n",
    "\n",
    "print(f'오디오 데이터 길이: {len(audio)}')\n",
    "print(f'샘플링 속도: {sampling_rate} Hz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1) 기본 파라미터로 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:   어떤 작업이든 지금부터 25분 동안 집중을 하고 5분 동안 휴식을 갖겠습니다. 앞으로 뽀모도로 기법을 활용해서 나의 하루를 바꿔나갈 것입니다. 나는 천재다. 나는 할 수 있다. 나는 좋은 일이 많이 생긴다. 나는 내가 원하는 모든 일을 이룰 수 있는 힘을 갖고 있다. 나는 준비가 되었고 나는 실행할 힘이 있다.\n"
     ]
    }
   ],
   "source": [
    "input_features = processor(\n",
    "    audio,\n",
    "    sampling_rate=16000,\n",
    "    return_tensors='pt'\n",
    ").input_features\n",
    "\n",
    "generates_ids = model.generate(input_features)\n",
    "transcription = processor.batch_decode(generates_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"Transcription: \", transcription[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) 하이퍼파라미터 튜닝 및 어텐션 마스크 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.3` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:   어떤 작업이든 지금부터 25분 동안 집중을 하고 5분 동안 휴식을 갖겠습니다. 앞으로 뽀모도로 기법을 활용해서 나의 하루를 바꿔나갈 것입니다. 나는 천재다. 나는 할 수 있다. 나는 좋은 일이 많이 생긴다. 나는 내가 원하는 모든 일을 이룰 수 있는 힘을 갖고 있다. 나는 준비가 되었고, 나는 실행할 힘이 있다.\n"
     ]
    }
   ],
   "source": [
    "attention_mask = torch.ones_like(input_features)  # 모든 입력이 활성화된 상태로 설정\n",
    "\n",
    "generates_ids = model.generate(\n",
    "    input_features,\n",
    "    temperature=0.7, # 다양성\n",
    "    length_penalty=1.3, # 기본값 1, 긴 텍스트에 조금 더 유리하도록 설정\n",
    "    attention_mask=attention_mask, # 입력 데이터에 대해 명시적으로 attention_mask 생성 후 전달\n",
    ")\n",
    "transcription = processor.batch_decode(generates_ids, skip_special_tokens=True)\n",
    "print(\"Transcription: \", transcription[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "# .env 파일에서 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "# Hugging Face 토큰 가져오기\n",
    "hf_token = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "\n",
    "# Hugging Face Access Token 입력\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'bartowski/gemma-2-9b-it-GGUF'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bartowski/gemma-2-9b-it-GGUF' is the correct path to a directory containing all relevant files for a GemmaTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# bnb_config = BitsAndBytesConfig(\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#     _load_in_4bit=True,\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#     # quant_method='bnb'\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m     15\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbartowski/gemma-2-9b-it-GGUF\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 16\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m llm \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     18\u001b[0m     pretrained_model_name_or_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTheBloke/Llama-2-9B-GGUF\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m     model_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-2-9b-it-Q3_K_L.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     20\u001b[0m     model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# gpu_layers=50\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# model = AutoModelForCausalLM.from_pretrained(\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#     pretrained_model_name_or_path=model_name,\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#     device_map='auto',\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 테스트 입력\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:940\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    937\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 940\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    942\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2016\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2013\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[0;32m   2014\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[0;32m   2015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[1;32m-> 2016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2019\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2021\u001b[0m     )\n\u001b[0;32m   2023\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'bartowski/gemma-2-9b-it-GGUF'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bartowski/gemma-2-9b-it-GGUF' is the correct path to a directory containing all relevant files for a GemmaTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import BitsAndBytesConfig\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# 4-bit 양자화된 모델 로드\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     _load_in_4bit=True,\n",
    "#     # quant_method='bnb'\n",
    "# )\n",
    "\n",
    "model_name = \"bartowski/gemma-2-9b-it-GGUF\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"TheBloke/Llama-2-9B-GGUF\",\n",
    "    model_file=\"gemma-2-9b-it-Q3_K_L.gguf\", \n",
    "    model_type=\"llama\", \n",
    "    # gpu_layers=50\n",
    "    )\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     pretrained_model_name_or_path=model_name,\n",
    "#     device_map='auto',\n",
    "#     )\n",
    "\n",
    "# 테스트 입력\n",
    "# inputs = tokenizer(transcription, return_tensors=\"pt\")\n",
    "# outputs = model.generate(inputs[\"input_ids\"], max_length=510, num_return_sequences=1)\n",
    "\n",
    "# 결과 확인\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not load model bartowski/gemma-2-9b-it-GGUF with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 3929, in from_pretrained\n    raise EnvironmentError(\nOSError: bartowski/gemma-2-9b-it-GGUF does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbartowski/gemma-2-9b-it-GGUF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemma-2-9b-it-Q3_K_L.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:940\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    939\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 940\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    950\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    951\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\pipelines\\base.py:302\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    301\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    303\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         )\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model bartowski/gemma-2-9b-it-GGUF with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_영어대화-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 3929, in from_pretrained\n    raise EnvironmentError(\nOSError: bartowski/gemma-2-9b-it-GGUF does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"bartowski/gemma-2-9b-it-GGUF\",\n",
    "    model_name=\"gemma-2-9b-it-Q3_K_L.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())  # True여야 GPU가 활성화된 상태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.20.1+cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "torchvision.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_영어대화-HAZ4-uhy-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
