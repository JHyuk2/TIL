{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ëª…\n",
    "\n",
    "- ì˜¤ë””ì˜¤ ì²˜ë¦¬: numpy, librosa, soundfile, pydub\n",
    "- ëª¨ë¸ ì‹¤í–‰: torch, transformers, accelerate\n",
    "- ì‘ìš© í”„ë¡œê·¸ë¨ êµ¬ì„±: langchain, sentence-transformers\n",
    "- ë°ì´í„°ì…‹: datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì„¤ì¹˜í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ ì„¤ëª…  \n",
    "\n",
    "### í•µì‹¬ AI ë° NLP ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "1. `torch`  \n",
    "PyTorchëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  í›ˆë ¨í•˜ëŠ” ë° ì‚¬ìš©í•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.  \n",
    "GPU ê°€ì†ì„ ì§€ì›í•˜ë©°, íŠ¹íˆ ìì—°ì–´ ì²˜ë¦¬ì™€ ì»´í“¨í„° ë¹„ì „ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
    "Transformers ëª¨ë¸ì„ ì‹¤í–‰í•˜ê±°ë‚˜ ì»¤ìŠ¤í…€ ëª¨ë¸ì„ í›ˆë ¨í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
    "\n",
    "2. `transformers`  \n",
    "Hugging Faceì—ì„œ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ì‚¬ì „ í•™ìŠµëœ NLP ëª¨ë¸(BERT, GPT, Whisper ë“±)ì„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "Whisper ëª¨ë¸ë„ ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.  \n",
    "Whisper ëª¨ë¸ë¡œ ìŒì„± ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
    "\n",
    "3. `accelerate`  \n",
    "Hugging Faceì—ì„œ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ëª¨ë¸ í›ˆë ¨ê³¼ ì¶”ë¡ ì„ ê°€ì†í™”í•˜ê³  ì—¬ëŸ¬ ë””ë°”ì´ìŠ¤(CPU, GPU, TPU ë“±)ë¥¼ í™œìš©í•˜ë„ë¡ ì§€ì›í•©ë‹ˆë‹¤.  \n",
    "Whisperë‚˜ ê¸°íƒ€ Transformer ê¸°ë°˜ ëª¨ë¸ì„ ì‹¤í–‰ ì‹œ ì„±ëŠ¥ ìµœì í™”ì— ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
    "\n",
    "4. `langchain`  \n",
    "ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.  \n",
    "ì—¬ëŸ¬ NLP ì‘ì—…(ì§ˆì˜ì‘ë‹µ, ëŒ€í™” ìƒì„± ë“±)ì„ ì—°ê²°í•˜ëŠ” ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì„±í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
    "LLM ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì— ì‚¬ìš©ë©ë‹ˆë‹¤. Whisperì™€ ê²°í•©í•˜ì—¬ ìŒì„± ì¸ì‹ í›„ ì²˜ë¦¬ ë¡œì§ì„ ì‘ì„±í•  ë•Œ ìœ ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "\n",
    "1. `sentence-transformers`  \n",
    "ë¬¸ì¥ ìˆ˜ì¤€ì˜ ì„ë² ë”©(ë²¡í„° í‘œí˜„)ì„ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.  \n",
    "í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ìœ ì‚¬ë„ ì¸¡ì •ì´ë‚˜ ê²€ìƒ‰ ì‘ì—…ì— ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
    "Whisperë¡œ ë³€í™˜ëœ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê±°ë‚˜ ë¶„ì„í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "\n",
    "\n",
    "### ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë° ë°ì´í„° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬  \n",
    "1. `numpy==1.23.4`  \n",
    "Pythonì˜ ëŒ€í‘œì ì¸ ìˆ˜ì¹˜ ê³„ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, í–‰ë ¬ ì—°ì‚° ë° ê³ ì„±ëŠ¥ ë°°ì—´ ì²˜ë¦¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.  \n",
    "Whisper ëª¨ë¸ ë° ì˜¤ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬ì—ì„œ í•µì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.  \n",
    "PCM ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê±°ë‚˜ librosaì™€ í•¨ê»˜ ì˜¤ë””ì˜¤ ë°ì´í„° ë°°ì—´ì„ ì¡°ì‘í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
    "(`librosa`ì™€ í˜¸í™˜í•˜ê¸° ìœ„í•´ ë²„ì „ì„ ì§€ì •í•´ì£¼ì—ˆìŠµë‹ˆë‹¤. 2.0.0 ë²„ì „ë„ ê°€ëŠ¥)  \n",
    "\n",
    "2. `librosa`  \n",
    "ì˜¤ë””ì˜¤ ë¶„ì„ê³¼ ì‹ í˜¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.  \n",
    "ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ì£¼íŒŒìˆ˜ ì˜ì—­(Mel Spectrogram)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë“± Whisper ëª¨ë¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ì—°ê´€ì´ ìˆìŠµë‹ˆë‹¤.  \n",
    "Whisper ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ì œê³µë˜ëŠ” ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê±°ë‚˜ ë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
    "\n",
    "3. `soundfile`  \n",
    "ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì½ê³  ì“°ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.  \n",
    "`librosa`ê°€ ë‚´ë¶€ì ìœ¼ë¡œ ì˜ì¡´í•©ë‹ˆë‹¤.  \n",
    "ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê±°ë‚˜ ì €ì¥í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
    "\n",
    "4. `pydub`  \n",
    "ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ìë¥´ê±°ë‚˜ í•©ì¹˜ëŠ” ë“±ì˜ ì‘ì—…ì„ ì§€ì›í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.  \n",
    "ffmpegì™€ í•¨ê»˜ ë™ì‘í•˜ë©°, Whisperì— ì…ë ¥ìœ¼ë¡œ ì œê³µí•  ì˜¤ë””ì˜¤ë¥¼ ì¤€ë¹„í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.  \n",
    "ê¸´ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ 30ì´ˆ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê±°ë‚˜ íŠ¹ì • í¬ë§·ìœ¼ë¡œ ë³€í™˜í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers accelerate langchain sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.23.4 librosa soundfile pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„°ì…‹ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Ipython Notebookì—ì„œ ë…¹ìŒ ë²„íŠ¼ UI êµ¬í˜„í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import threading\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0 Microsoft Sound Mapper - Output, MME (0 in, 2 out)\n",
       "< 1 ìŠ¤í”¼ì»¤(Realtek High Definition Aud, MME (0 in, 8 out)\n",
       "  2 ì£¼ ì‚¬ìš´ë“œ ë“œë¼ì´ë²„, Windows DirectSound (0 in, 2 out)\n",
       "  3 ìŠ¤í”¼ì»¤(Realtek High Definition Audio), Windows DirectSound (0 in, 8 out)\n",
       "  4 ìŠ¤í”¼ì»¤(Realtek High Definition Audio), Windows WASAPI (0 in, 2 out)\n",
       "  5 ë§ˆì´í¬ (Realtek HD Audio Mic input), Windows WDM-KS (2 in, 0 out)\n",
       "  6 ìŠ¤í…Œë ˆì˜¤ ë¯¹ìŠ¤ (Realtek HD Audio Stereo input), Windows WDM-KS (2 in, 0 out)\n",
       "  7 ë¼ì¸ ì…ë ¥ (Realtek HD Audio Line input), Windows WDM-KS (2 in, 0 out)\n",
       "  8 Speakers (Realtek HD Audio output), Windows WDM-KS (0 in, 8 out)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd.query_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0 Microsoft Sound Mapper - Output, MME (0 in, 2 out)\n",
       "< 1 ìŠ¤í”¼ì»¤(Realtek High Definition Aud, MME (0 in, 8 out)\n",
       "  2 ì£¼ ì‚¬ìš´ë“œ ë“œë¼ì´ë²„, Windows DirectSound (0 in, 2 out)\n",
       "  3 ìŠ¤í”¼ì»¤(Realtek High Definition Audio), Windows DirectSound (0 in, 8 out)\n",
       "  4 ìŠ¤í”¼ì»¤(Realtek High Definition Audio), Windows WASAPI (0 in, 2 out)\n",
       "  5 ë§ˆì´í¬ (Realtek HD Audio Mic input), Windows WDM-KS (2 in, 0 out)\n",
       "  6 ìŠ¤í…Œë ˆì˜¤ ë¯¹ìŠ¤ (Realtek HD Audio Stereo input), Windows WDM-KS (2 in, 0 out)\n",
       "  7 ë¼ì¸ ì…ë ¥ (Realtek HD Audio Line input), Windows WDM-KS (2 in, 0 out)\n",
       "  8 Speakers (Realtek HD Audio output), Windows WDM-KS (0 in, 8 out)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd.query_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë…¹ìŒ ê´€ë ¨ ë³€ìˆ˜\n",
    "recording = False\n",
    "paused = False\n",
    "audio_data = []\n",
    "sample_rate = 16000 # Whisper ê¶Œì¥ ìƒ˜í”Œë§ ë ˆì´íŠ¸\n",
    "sample_number = 1\n",
    "elapsed_time = 0 # ê²½ê³¼ ì‹œê°„ (ì´ˆ)\n",
    "timer_running = False # íƒ€ì´ë¨¸ ì‹¤í–‰ ì—¬ë¶€\n",
    "\n",
    "# ë…¹ìŒ í•¨ìˆ˜\n",
    "def record_audio():\n",
    "    global recording, paused, audio_data\n",
    "    audio_data = []\n",
    "\n",
    "    with sd.InputStream(samplerate=sample_rate, channels=1, dtype='int16') as stream:\n",
    "        while recording:\n",
    "            if not paused:\n",
    "                frame, _ = stream.read(1024)\n",
    "                audio_data.append(frame)\n",
    "\n",
    "# íƒ€ì´ë¨¸ í•¨ìˆ˜\n",
    "def update_timer():\n",
    "    global elapsed_time, timer_running\n",
    "    timer_running = True\n",
    "    while timer_running:\n",
    "        time.sleep(1)\n",
    "        if recording and not paused:\n",
    "            elapsed_time += 1\n",
    "            timer_label.value = f\"â³ ê²½ê³¼ ì‹œê°„: {elapsed_time}ì´ˆ\"\n",
    "\n",
    "# ë…¹ìŒ ì‹œì‘ í•¨ìˆ˜\n",
    "def start_recording(_):\n",
    "    global recording, paused, elapsed_time, timer_running\n",
    "    if not recording:\n",
    "        recording=True\n",
    "        paused = False\n",
    "        elapsed_time = 0 # íƒ€ì´ë¨¸ ì´ˆê¸°í™”\n",
    "        threading.Thread(target=record_audio, daemon=True).start()\n",
    "        threading.Thread(target=update_timer, daemon=True).start()\n",
    "        status_label.value = \"ğŸ™ë…¹ìŒ ì¤‘...\"\n",
    "        timer_label.value = \"ê²½ê³¼ ì‹œê°„: 0ì´ˆ\"\n",
    "\n",
    "# ì¼ì‹œì •ì§€ í•¨ìˆ˜\n",
    "def pause_recording(_):\n",
    "    global paused\n",
    "    if recording:\n",
    "        paused = not paused\n",
    "        status_label.value = \"â¸ ë…¹ìŒ ì¼ì‹œì •ì§€ë¨...\" if paused else \"ğŸ™ ë…¹ìŒ ì¬ê°œë¨...\"\n",
    "\n",
    "\n",
    "# ë…¹ìŒ ì¤‘ì§€ í•¨ìˆ˜\n",
    "def stop_recording(_):\n",
    "    global recording, timer_running\n",
    "    if recording:\n",
    "        recording = False\n",
    "        timer_running = False\n",
    "        status_label.value = \"âœ… ë…¹ìŒ ì™„ë£Œ!\"\n",
    "        save_audio()\n",
    "        timer_label.value = \"ê²½ê³¼ ì‹œê°„: 0ì´ˆ\"\n",
    "\n",
    "\n",
    "# ë…¹ìŒëœ íŒŒì¼ ì €ì¥ í•¨ìˆ˜\n",
    "def save_audio():\n",
    "    global sample_number\n",
    "    if len(audio_data) > 0:\n",
    "        audio_array = np.concatenate(audio_data, axis=0)\n",
    "        audio_name = f\"recorded_audio_{sample_number}.wav\"\n",
    "        sf.write(audio_name, audio_array, sample_rate)\n",
    "        sample_number += 1\n",
    "        status_label.value = f\"ğŸ¤ ë…¹ìŒ íŒŒì¼ ì €ì¥ë¨: {audio_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc47340ffb54b4a919c34587c32955a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='ë…¹ìŒ ì‹œì‘', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c501f546a0442f9821789815310ab00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='warning', description='ì¼ì‹œì •ì§€', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab2d2026fbb4197befd297e2b7b91be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='danger', description='ë…¹ìŒ ì¢…ë£Œ', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40df42835f9f429d9446d4d4a041ccb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='ê²½ê³¼ ì‹œê°„: 0ì´ˆ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13dfe847b5ba43b09bf11c8fbfb91e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-597 (record_audio):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\win\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\win\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\win\\AppData\\Local\\Temp\\ipykernel_21120\\2581066454.py\", line 15, in record_audio\n",
      "  File \"c:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12\\Lib\\site-packages\\sounddevice.py\", line 1440, in __init__\n",
      "    _StreamBase.__init__(self, kind='input', wrap_callback='array',\n",
      "  File \"c:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12\\Lib\\site-packages\\sounddevice.py\", line 828, in __init__\n",
      "    _get_stream_parameters(kind, device, channels, dtype, latency,\n",
      "  File \"c:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12\\Lib\\site-packages\\sounddevice.py\", line 2708, in _get_stream_parameters\n",
      "    info = query_devices(device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12\\Lib\\site-packages\\sounddevice.py\", line 572, in query_devices\n",
      "    raise PortAudioError(f'Error querying device {device}')\n",
      "sounddevice.PortAudioError: Error querying device -1\n"
     ]
    }
   ],
   "source": [
    "# UI ë²„íŠ¼ ìƒì„±\n",
    "start_button = widgets.Button(description=\"ë…¹ìŒ ì‹œì‘\", button_style=\"success\")\n",
    "pause_button = widgets.Button(description=\"ì¼ì‹œì •ì§€\", button_style=\"warning\")\n",
    "stop_button = widgets.Button(description=\"ë…¹ìŒ ì¢…ë£Œ\", button_style=\"danger\")\n",
    "status_label = widgets.Label(value=\"\")\n",
    "timer_label = widgets.Label(value=\"ê²½ê³¼ ì‹œê°„: 0ì´ˆ\")\n",
    "\n",
    "# ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸ ì—°ê²°\n",
    "start_button.on_click(start_recording)\n",
    "pause_button.on_click(pause_recording)\n",
    "stop_button.on_click(stop_recording)\n",
    "\n",
    "# UI í‘œì‹œ\n",
    "display(start_button, pause_button, stop_button, timer_label, status_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. ë°ì´í„°ì…‹ í™œìš©\n",
    "- datasets / distil-whisper/librispeech_long\n",
    "  - í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ì œê³µí•˜ëŠ” ìŒì„± ë°ì´í„°ì…‹ìœ¼ë¡œ, ì£¼ë¡œ ASRí‰ê°€ë¥¼ ìœ„í•´ ì‚¬ìš©\n",
    "  - ë°ì´í„° ìƒ˜í”Œ\n",
    "  \n",
    "  ```python\n",
    "  {\n",
    "    \"audio\": {\n",
    "        \"path\": \"íŒŒì¼ ê²½ë¡œ ë˜ëŠ” íŒŒì¼ ID\",\n",
    "        \"array\": array([...]),  # PCM ë°ì´í„° ë°°ì—´\n",
    "        \"sampling_rate\": 16000  # ìƒ˜í”Œë§ ì†ë„\n",
    "    }\n",
    "  }\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[0]['audio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. ë°ì´í„°ì…‹ ë³€í™˜ (PCM -> pydub.AudioSegment)\n",
    "- PCM(`Pulse Code Modulation`): ë””ì§€í„¸ ì˜¤ë””ì˜¤ ë°ì´í„°ì˜ ê¸°ë³¸ í˜•ì‹  \n",
    "\n",
    "- **íŠ¹ì§•**\n",
    "  - 1. ë¹„ì••ì¶• ë°ì´í„°\n",
    "    - ì••ì¶• ì½”ë±(MP3, AAC)ê³¼ ë‹¬ë¦¬ ì••ì¶•ë˜ì§€ ì•Šì€ ì›ì‹œì˜ ë°ì´í„°\n",
    "  - 2. ìƒ˜í”Œë§\n",
    "    - ì•„ë‚ ë¡œê·¸ ì˜¤ë””ì˜¤ ì‹ í˜¸ë¥¼ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¸¡ì •í•˜ì—¬ ë””ì§€í„¸ ê°’ìœ¼ë¡œ ë³€í™˜\n",
    "  - 3. ìƒ˜í”Œ í­\n",
    "    - PCMì´ ì°¨ì§€í•˜ëŠ” ë¹„íŠ¸ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ”ë°, 16ë¹„íŠ¸, 8ë¹„íŠ¸ ê°™ì€ ê²Œ ìˆìŒ.\n",
    "  - 4. ì±„ë„\n",
    "    - ì˜¤ë””ì˜¤ê°€ ëª¨ë…¸(1ì±„ë„)ì¸ì§€, ìŠ¤í…Œë ˆì˜¤(2ì±„ë„)ì¸ì§€ ê²°ì •.\n",
    "\n",
    "- **ì¥ë‹¨ì **\n",
    "  - ì¥ì \n",
    "    - 1. ê³ í’ˆì§ˆ ë°ì´í„°\n",
    "      - ì••ì¶•ë˜ì§€ ì•Šì€ ì›ë³¸ ë°ì´í„°ë¥¼ ì œê³µí•˜ë¯€ë¡œ, í’ˆì§ˆ ì†ì‹¤ì´ ì—†ìŒ\n",
    "    - 2. í‘œì¤€í™”\n",
    "      - ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë‚˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥\n",
    "  - ë‹¨ì \n",
    "    - 1. í° íŒŒì¼ í¬ê¸°\n",
    "      - ì••ì¶•ë˜ì§€ ì•Šì•˜ê¸°ì— í¬ê¸°ê°€ í¼\n",
    "    - 2. ì§ì ‘ ì‚¬ìš©ì— ë¶ˆí¸\n",
    "      - ì‚¬ëŒì´ ë°”ë¡œ ì´í•´í•˜ê±°ë‚˜ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ í˜•íƒœë¡œ, ì¶”ê°€ ì²˜ë¦¬ê°€ í•„ìš”í•¨.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì§§ê²Œ ë³´ë©´\n",
    "\n",
    "```python\n",
    "{\n",
    "    'audio':{\n",
    "        'path': \"íŒŒì¼ ê²½ë¡œ ë˜ëŠ” íŒŒì¼ ID\",\n",
    "        'array' : [0.0023, 0.0035, -0.0046, ...], #PCM ë°ì´í„° ë°°ì—´\n",
    "        'sampling_rate' : 16000, # ìƒ˜í”Œë§ ì†ë„ (16,000Hz)\n",
    "        'channels' : 1, # ëª¨ë…¸(1ì±„ë„)\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '0d38672e0bbdbdc460af55b8bb84a15b2730db2819f2af64f9c777d4d586f2de',\n",
       " 'array': array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00024414, 0.00048828,\n",
       "        0.0005188 ]),\n",
       " 'sampling_rate': 16000}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ\n",
    "\n",
    "# ì˜¤ë””ì˜¤ ë°ì´í„° ì¶”ì¶œ ë° AudioSegment ë³€í™˜\n",
    "audio_array = sample['array']\n",
    "sampling_rate = sample['sampling_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ì‹¤ì œë¡œ í•œ ë²ˆ ë“¤ì–´ë³´ì!\n",
    "---\n",
    "#### 1.2.1 Audioë¥¼ 30ì´ˆë§ˆë‹¤ ì²­í‚¹í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_0.wav ì €ì¥ ì™„ë£Œ\n",
      "chunk_1.wav ì €ì¥ ì™„ë£Œ\n",
      "chunk_2.wav ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# PCM ë°ì´í„°ë¥¼ 16ë¹„íŠ¸ ì •ìˆ˜ë¡œ ë³€í™˜\n",
    "audio_data = np.array(audio_array * 32767, dtype=np.int16)\n",
    "\n",
    "# AudioSegmentë¡œ ë³€í™˜\n",
    "audio = AudioSegment(\n",
    "    audio_data.tobytes(),\n",
    "    frame_rate=sampling_rate,\n",
    "    sample_width=audio_data.dtype.itemsize,\n",
    "    channels=1\n",
    ")\n",
    "\n",
    "# ì˜¤ë””ì˜¤ ë¶„í•  (30ì´ˆ ë‹¨ìœ„)\n",
    "chunk_length_ms = 30 * 1000  # 30ì´ˆ\n",
    "chunks = [audio[i:i + chunk_length_ms] for i in range(0, len(audio), chunk_length_ms)]\n",
    "\n",
    "# ê° ë¶„í•  ì €ì¥\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.export(f\"chunk_{i}.wav\", format=\"wav\")\n",
    "    print(f\"chunk_{i}.wav ì €ì¥ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pydub.audio_segment.AudioSegment at 0x256f89463f0>,\n",
       " <pydub.audio_segment.AudioSegment at 0x256f535b200>,\n",
       " <pydub.audio_segment.AudioSegment at 0x256f8944260>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Long-form ì‚¬ìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ë° íŒŒì´í”„ë¼ì¸ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "c:\\Users\\win\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.3` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from transformers.models.whisper import EncoderDecoderCache\n",
    "\n",
    "# CUDA ì‚¬ìš©\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Whisper ëª¨ë¸ ë¡œë“œ\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch_dtype,\n",
    "    # low_cpu_mem_usage=True, \n",
    "    use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# PCM ë°ì´í„°ë¥¼ Mel Spectorgramìœ¼ë¡œ ë³€í™˜ í›„ ì§„í–‰\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-large-v3-turbo\") # PCM -> Mel Spectrogram ì…ë ¥ ë³€í™˜ì„ ìœ„í•¨\n",
    "\n",
    "input_features = processor(\n",
    "    audio_array,\n",
    "    sampling_rate=sampling_rate,\n",
    "    return_tensors=\"pt\",\n",
    ").input_features\n",
    "\n",
    "\n",
    "# ì‹ ê·œ ë²„ì „ ì—…ë°ì´íŠ¸) ê¸°ì¡´ íŠœí”Œì„ EncoderDecoderCacheë¡œ ë³€í™˜\n",
    "# past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n",
    "attention_mask = torch.ones_like(input_features)  # ëª¨ë“  ì…ë ¥ì´ í™œì„±í™”ëœ ìƒíƒœë¡œ ì„¤ì •\n",
    "\n",
    "generates_ids = model.generate(\n",
    "    input_features,\n",
    "    temperature=0.7, # ë‹¤ì–‘ì„±\n",
    "    num_beams=2, # beam searchë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "    length_penalty=1.3, # ê¸°ë³¸ê°’ 1, ê¸´ í…ìŠ¤íŠ¸ì— ì¡°ê¸ˆ ë” ìœ ë¦¬í•˜ë„ë¡ ì„¤ì •\n",
    "    attention_mask=attention_mask, # ì…ë ¥ ë°ì´í„°ì— ëŒ€í•´ ëª…ì‹œì ìœ¼ë¡œ attention_mask ìƒì„± í›„ ì „ë‹¬\n",
    ")\n",
    "transcription = processor.batch_decode(generates_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: [\" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind.\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"Transcription:\", transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ë…¹ìŒí•œ íŒŒì¼ë¡œ ASR í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\dm705\\\\Study\\\\TIL\\\\Projects\\\\AI ì˜ì–´ëŒ€í™”'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: True\n"
     ]
    }
   ],
   "source": [
    "file_name = 'self_motivation.m4a'\n",
    "file_path = os.path.join(os.getcwd(), file_name)\n",
    "print(\"íŒŒì¼ ì¡´ì¬ ì—¬ë¶€:\", os.path.exists(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAV ë³€í™˜ ì„±ê³µ\n"
     ]
    }
   ],
   "source": [
    "# ì´ë¯¸ ë§Œë“¤ì–´ì§„ processor, modelì„ ì‚¬ìš©\n",
    "\n",
    "'''\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch_dtype,\n",
    "    # low_cpu_mem_usage=True, \n",
    "    use_safetensors=True\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-large-v3-turbo\") # PCM -> Mel Spectrogram ì…ë ¥ ë³€í™˜ì„ ìœ„í•¨\n",
    "'''\n",
    "\n",
    "# Whisperëª¨ë¸ì€ PCMë°ì´í„° or WAVí˜•ì‹ ë°ì´í„°ë¥¼ í•„ìš”ë¡œ í•˜ê¸° ë•Œë¬¸ì—, ë³€í™˜í•´ì¤Œ.\n",
    "import traceback # ì „ì²´ ì˜¤ë¥˜ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤\n",
    "\n",
    "# load file\n",
    "try:\n",
    "    record_file = AudioSegment.from_file(file_path, format=\"m4a\")\n",
    "    # convert to WAV format\n",
    "    record_file.export(\"self_motivation.wav\", format='wav')\n",
    "    print(\"WAV ë³€í™˜ ì„±ê³µ\")\n",
    "except Exception as e:\n",
    "    print(\"ì˜¤ë””ì˜¤ ë³€í™˜ ì‹¤íŒ¨:\", e)\n",
    "    traceback.print_exc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FileNotFoundErrorê°€ ë°œìƒí•˜ëŠ” ê²½ìš°  \n",
    "> traceback.print_exc() ë¥¼ ì‚¬ìš©í•´ì„œ, ì „ì²´ ì˜¤ë¥˜ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ë¥¼ ì¶œë ¥í•  ìˆ˜ ìˆë‹¤.    \n",
    "> ë‚˜ì˜ ê²½ìš° `pydub`ì´ ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” `FFmpeg`ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ëŠ” ê²ƒ ê°™ì•˜ìŒ.  \n",
    "> - `Couldn't find ffprobe or avprobe` warningì´ ë°œìƒí–ˆê¸° ë•Œë¬¸.  \n",
    "> ë•Œë¬¸ì— `FFmpeg`ë¥¼ ì•„ë˜ì™€ ê°™ì´ ì„¤ì¹˜í•´ì¤Œ.\n",
    "\n",
    "---\n",
    "#### 1. FFmpeg ë‹¤ìš´ë¡œë“œ\n",
    "- [FFmpeg ê³µì‹ ì›¹ì‚¬ì´íŠ¸ ì ‘ì†](https://ffmpeg.org/download.html)\n",
    "- Windows ë¹Œë“œ ì„¹ì…˜ì—ì„œ \"Windows builds by gyan.dev\" ë§í¬ í´ë¦­.\n",
    "#### 2. Windows ë¹Œë“œ ë‹¤ìš´ë¡œë“œ\n",
    "- `ffmpeg-git-full.7z` íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
    "#### 3. ì••ì¶• í•´ì œ í›„ í™˜ê²½ë³€ìˆ˜ ì„¤ì •\n",
    "- ì••ì¶•í•œ í´ë”ì˜ `ffmpeg-bin` í´ë”ë¡œ ê°€ì„œ, í™˜ê²½ë³€ìˆ˜(ì‹œìŠ¤í…œë³€ìˆ˜)ë¡œ ì¶”ê°€.\n",
    "- bashì°½ì„ ë‹¤ì‹œ ë‹«ê³  ì•„ë˜ì˜ ëª…ë ¹ì–´ ì‹¤í–‰\n",
    "- ```bash\n",
    "    ffmpeg -version  \n",
    "    ffprobe -version\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAV ë³€í™˜ ì„±ê³µ\n"
     ]
    }
   ],
   "source": [
    "# ì¬ë„ì „\n",
    "\n",
    "# load file\n",
    "try:\n",
    "    record_file = AudioSegment.from_file(file_path, format=\"m4a\")\n",
    "    # convert to WAV format\n",
    "    record_file.export(\"self_motivation.wav\", format='wav')\n",
    "    print(\"WAV ë³€í™˜ ì„±ê³µ\")\n",
    "except Exception as e:\n",
    "    print(\"ì˜¤ë””ì˜¤ ë³€í™˜ ì‹¤íŒ¨:\", e)\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë””ì˜¤ ë°ì´í„° ê¸¸ì´: 492203\n",
      "ìƒ˜í”Œë§ ì†ë„: 16000 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dm705\\AppData\\Local\\Temp\\ipykernel_15856\\1433415770.py:2: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sampling_rate = librosa.load(file_path, sr=16000)\n",
      "c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    }
   ],
   "source": [
    "# librosaë¥¼ ì´ìš©í•œ íŒŒì¼ ì½ê¸°\n",
    "audio, sampling_rate = librosa.load(file_path, sr=16000)\n",
    "\n",
    "print(f'ì˜¤ë””ì˜¤ ë°ì´í„° ê¸¸ì´: {len(audio)}')\n",
    "print(f'ìƒ˜í”Œë§ ì†ë„: {sampling_rate} Hz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1) ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¡œ ì§„í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:   ì–´ë–¤ ì‘ì—…ì´ë“  ì§€ê¸ˆë¶€í„° 25ë¶„ ë™ì•ˆ ì§‘ì¤‘ì„ í•˜ê³  5ë¶„ ë™ì•ˆ íœ´ì‹ì„ ê°–ê² ìŠµë‹ˆë‹¤. ì•ìœ¼ë¡œ ë½€ëª¨ë„ë¡œ ê¸°ë²•ì„ í™œìš©í•´ì„œ ë‚˜ì˜ í•˜ë£¨ë¥¼ ë°”ê¿”ë‚˜ê°ˆ ê²ƒì…ë‹ˆë‹¤. ë‚˜ëŠ” ì²œì¬ë‹¤. ë‚˜ëŠ” í•  ìˆ˜ ìˆë‹¤. ë‚˜ëŠ” ì¢‹ì€ ì¼ì´ ë§ì´ ìƒê¸´ë‹¤. ë‚˜ëŠ” ë‚´ê°€ ì›í•˜ëŠ” ëª¨ë“  ì¼ì„ ì´ë£° ìˆ˜ ìˆëŠ” í˜ì„ ê°–ê³  ìˆë‹¤. ë‚˜ëŠ” ì¤€ë¹„ê°€ ë˜ì—ˆê³  ë‚˜ëŠ” ì‹¤í–‰í•  í˜ì´ ìˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "input_features = processor(\n",
    "    audio,\n",
    "    sampling_rate=16000,\n",
    "    return_tensors='pt'\n",
    ").input_features\n",
    "\n",
    "generates_ids = model.generate(input_features)\n",
    "transcription = processor.batch_decode(generates_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"Transcription: \", transcription[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë° ì–´í…ì…˜ ë§ˆìŠ¤í¬ ì œê³µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.3` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:   ì–´ë–¤ ì‘ì—…ì´ë“  ì§€ê¸ˆë¶€í„° 25ë¶„ ë™ì•ˆ ì§‘ì¤‘ì„ í•˜ê³  5ë¶„ ë™ì•ˆ íœ´ì‹ì„ ê°–ê² ìŠµë‹ˆë‹¤. ì•ìœ¼ë¡œ ë½€ëª¨ë„ë¡œ ê¸°ë²•ì„ í™œìš©í•´ì„œ ë‚˜ì˜ í•˜ë£¨ë¥¼ ë°”ê¿”ë‚˜ê°ˆ ê²ƒì…ë‹ˆë‹¤. ë‚˜ëŠ” ì²œì¬ë‹¤. ë‚˜ëŠ” í•  ìˆ˜ ìˆë‹¤. ë‚˜ëŠ” ì¢‹ì€ ì¼ì´ ë§ì´ ìƒê¸´ë‹¤. ë‚˜ëŠ” ë‚´ê°€ ì›í•˜ëŠ” ëª¨ë“  ì¼ì„ ì´ë£° ìˆ˜ ìˆëŠ” í˜ì„ ê°–ê³  ìˆë‹¤. ë‚˜ëŠ” ì¤€ë¹„ê°€ ë˜ì—ˆê³ , ë‚˜ëŠ” ì‹¤í–‰í•  í˜ì´ ìˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "attention_mask = torch.ones_like(input_features)  # ëª¨ë“  ì…ë ¥ì´ í™œì„±í™”ëœ ìƒíƒœë¡œ ì„¤ì •\n",
    "\n",
    "generates_ids = model.generate(\n",
    "    input_features,\n",
    "    temperature=0.7, # ë‹¤ì–‘ì„±\n",
    "    length_penalty=1.3, # ê¸°ë³¸ê°’ 1, ê¸´ í…ìŠ¤íŠ¸ì— ì¡°ê¸ˆ ë” ìœ ë¦¬í•˜ë„ë¡ ì„¤ì •\n",
    "    attention_mask=attention_mask, # ì…ë ¥ ë°ì´í„°ì— ëŒ€í•´ ëª…ì‹œì ìœ¼ë¡œ attention_mask ìƒì„± í›„ ì „ë‹¬\n",
    ")\n",
    "transcription = processor.batch_decode(generates_ids, skip_special_tokens=True)\n",
    "print(\"Transcription: \", transcription[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# Hugging Face í† í° ê°€ì ¸ì˜¤ê¸°\n",
    "hf_token = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "\n",
    "# Hugging Face Access Token ì…ë ¥\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'bartowski/gemma-2-9b-it-GGUF'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bartowski/gemma-2-9b-it-GGUF' is the correct path to a directory containing all relevant files for a GemmaTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# bnb_config = BitsAndBytesConfig(\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#     _load_in_4bit=True,\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#     # quant_method='bnb'\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m     15\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbartowski/gemma-2-9b-it-GGUF\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 16\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m llm \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     18\u001b[0m     pretrained_model_name_or_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTheBloke/Llama-2-9B-GGUF\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m     model_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-2-9b-it-Q3_K_L.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     20\u001b[0m     model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# gpu_layers=50\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# model = AutoModelForCausalLM.from_pretrained(\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#     pretrained_model_name_or_path=model_name,\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#     device_map='auto',\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# í…ŒìŠ¤íŠ¸ ì…ë ¥\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:940\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    937\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 940\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    942\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2016\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2013\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[0;32m   2014\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[0;32m   2015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[1;32m-> 2016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2019\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2021\u001b[0m     )\n\u001b[0;32m   2023\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'bartowski/gemma-2-9b-it-GGUF'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bartowski/gemma-2-9b-it-GGUF' is the correct path to a directory containing all relevant files for a GemmaTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import BitsAndBytesConfig\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# 4-bit ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     _load_in_4bit=True,\n",
    "#     # quant_method='bnb'\n",
    "# )\n",
    "\n",
    "model_name = \"bartowski/gemma-2-9b-it-GGUF\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"TheBloke/Llama-2-9B-GGUF\",\n",
    "    model_file=\"gemma-2-9b-it-Q3_K_L.gguf\", \n",
    "    model_type=\"llama\", \n",
    "    # gpu_layers=50\n",
    "    )\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     pretrained_model_name_or_path=model_name,\n",
    "#     device_map='auto',\n",
    "#     )\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì…ë ¥\n",
    "# inputs = tokenizer(transcription, return_tensors=\"pt\")\n",
    "# outputs = model.generate(inputs[\"input_ids\"], max_length=510, num_return_sequences=1)\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not load model bartowski/gemma-2-9b-it-GGUF with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 3929, in from_pretrained\n    raise EnvironmentError(\nOSError: bartowski/gemma-2-9b-it-GGUF does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbartowski/gemma-2-9b-it-GGUF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemma-2-9b-it-Q3_K_L.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:940\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    939\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 940\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    950\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    951\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\pipelines\\base.py:302\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    301\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    303\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         )\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model bartowski/gemma-2-9b-it-GGUF with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\dm705\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai_ì˜ì–´ëŒ€í™”-UlUCYZce-py3.12\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 3929, in from_pretrained\n    raise EnvironmentError(\nOSError: bartowski/gemma-2-9b-it-GGUF does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"bartowski/gemma-2-9b-it-GGUF\",\n",
    "    model_name=\"gemma-2-9b-it-Q3_K_L.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())  # Trueì—¬ì•¼ GPUê°€ í™œì„±í™”ëœ ìƒíƒœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.20.1+cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "torchvision.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_ì˜ì–´ëŒ€í™”-HAZ4-uhy-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
